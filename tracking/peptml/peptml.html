<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.6.3" />
<title>pept.tracking.peptml.peptml API documentation</title>
<meta name="description" content="The *peptml* package implements a hierarchical density-based clustering
algorithm for general Positron Emission Particle Tracking (PEPT) …" />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{font-weight:bold}#index h4 + ul{margin-bottom:.6em}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase;cursor:pointer}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>pept.tracking.peptml.peptml</code></h1>
</header>
<section id="section-intro">
<p>The <em>peptml</em> package implements a hierarchical density-based clustering
algorithm for general Positron Emission Particle Tracking (PEPT).</p>
<p>The package includes tools for PEPT data analysis and visualisation. It
exports
two main classes that are meant to work together: <a title="pept.tracking.peptml.peptml.Cutpoints" href="#pept.tracking.peptml.peptml.Cutpoints"><code>Cutpoints</code></a>
and <a title="pept.tracking.peptml.peptml.HDBSCANClusterer" href="#pept.tracking.peptml.peptml.HDBSCANClusterer"><code>HDBSCANClusterer</code></a>. <a title="pept.tracking.peptml.peptml.Cutpoints" href="#pept.tracking.peptml.peptml.Cutpoints"><code>Cutpoints</code></a> transforms the LoR data into <em>cutpoints</em> that
can then be clustered using <a title="pept.tracking.peptml.peptml.HDBSCANClusterer" href="#pept.tracking.peptml.peptml.HDBSCANClusterer"><code>HDBSCANClusterer</code></a>. <a title="pept.tracking.peptml.peptml.Cutpoints" href="#pept.tracking.peptml.peptml.Cutpoints"><code>Cutpoints</code></a> accepts any instance
of a class that inherits from <code>LineData</code> - this is a requirement due to the
low-level C-API that it calls. <a title="pept.tracking.peptml.peptml.HDBSCANClusterer" href="#pept.tracking.peptml.peptml.HDBSCANClusterer"><code>HDBSCANClusterer</code></a> can work with both <code>PointData</code>,
as well as typical <code>numpy.ndarray</code>s.</p>
<p>The module aims to provide general classes which can
then be used in a script file as the user sees fit. For example scripts,
look at the base of the pept library.</p>
<p><code>peptml</code> requires the following packages:</p>
<ul>
<li><strong>numpy</strong></li>
<li><strong>joblib</strong> for multithreaded operations (such as cutpoints-finding)</li>
<li><strong>tqdm</strong> for showing progress bars</li>
<li><strong>plotly &gt;= 4.1.0</strong> for plotly-based plotting</li>
<li><strong>hdbscan</strong> for clustering cutpoints and centres</li>
<li><strong>time</strong> for verbose timing of operations</li>
</ul>
<p>It was successfuly used at the University of Birmingham to analyse real
Fluorine-18 tracers in air.</p>
<p>If you use this package, you should cite
the following paper: [TODO: paper signature].</p>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">#!/usr/bin/env python3
# -*- coding: utf-8 -*-


#    pept is a Python library that unifies Positron Emission Particle
#    Tracking (PEPT) research, including tracking, simulation, data analysis
#    and visualisation tools
#
#    Copyright (C) 2019 Andrei Leonard Nicusan
#
#    This program is free software: you can redistribute it and/or modify
#    it under the terms of the GNU General Public License as published by
#    the Free Software Foundation, either version 3 of the License, or
#    (at your option) any later version.
#
#    This program is distributed in the hope that it will be useful,
#    but WITHOUT ANY WARRANTY; without even the implied warranty of
#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#    GNU General Public License for more details.
#
#    You should have received a copy of the GNU General Public License
#    along with this program.  If not, see &lt;https://www.gnu.org/licenses/&gt;.


# File   : peptml.py
# License: License: GNU v3.0
# Author : Andrei Leonard Nicusan &lt;a.l.nicusan@bham.ac.uk&gt;
# Date   : 28.08.2019


&#39;&#39;&#39;The *peptml* package implements a hierarchical density-based clustering
algorithm for general Positron Emission Particle Tracking (PEPT).

The package includes tools for PEPT data analysis and visualisation. It  exports
two main classes that are meant to work together: `Cutpoints`
and `HDBSCANClusterer`. `Cutpoints` transforms the LoR data into *cutpoints* that
can then be clustered using `HDBSCANClusterer`. `Cutpoints` accepts any instance
of a class that inherits from `LineData` - this is a requirement due to the
low-level C-API that it calls. `HDBSCANClusterer` can work with both `PointData`,
as well as typical `numpy.ndarray`s.

The module aims to provide general classes which can
then be used in a script file as the user sees fit. For example scripts,
look at the base of the pept library.

`peptml` requires the following packages:

* **numpy**
* **joblib** for multithreaded operations (such as cutpoints-finding)
* **tqdm** for showing progress bars
* **plotly &gt;= 4.1.0** for plotly-based plotting
* **hdbscan** for clustering cutpoints and centres
* **time** for verbose timing of operations

It was successfuly used at the University of Birmingham to analyse real
Fluorine-18 tracers in air.

If you use this package, you should cite
the following paper: [TODO: paper signature].

&#39;&#39;&#39;


import  time
import  sys

import  numpy                                   as          np
from    scipy.spatial                           import      cKDTree

from    joblib                                  import      Parallel,       delayed
from    tqdm                                    import      tqdm
from    plotly.subplots                         import      make_subplots
import  plotly.graph_objects                    as          go

# Fix a deprecation warning inside the sklearn library
try:
    sys.modules[&#39;sklearn.externals.six&#39;] = __import__(&#39;six&#39;)
    sys.modules[&#39;sklearn.externals.joblib&#39;] = __import__(&#39;joblib&#39;)
    import hdbscan
except ImportError:
    import hdbscan

import  pept
from    .extensions.find_cutpoints_api          import      find_cutpoints_api




class Cutpoints(pept.PointData):
    &#39;&#39;&#39;A class that transforms LoRs into *cutpoints* for clustering.

    The `Cutpoints` class transforms LoRs (individual numpy arrays or full `LineData`)
    into cutpoints (individual numpy arrays or full `PointData`) that can then be passed
    to `HDBSCANClusterer`.

    Under typical usage, the class is instantiated with the LoR data (required to be
    an instance of `LineData`) and transforms it into an instance of `PointData` that
    stores the found cutpoints.

    For more control over the operations, the class also provides a static method (i.e.
    it can be used without instantiating the class) `find_cutpoints_sample` that
    receives a generic numpy array of LoRs (one &#39;sample&#39;) and returns a numpy array
    of cutpoints.

    Parameters
    ----------
    line_data : instance of pept.LineData
        The LoRs for which the cutpoints will be computed. It is required to be an
        instance of `pept.LineData`.
    max_distance : float
        The maximum distance between any two lines so that their cutpoint will be
        considered.
    cutoffs : list-like of length 6, optional
        A list (or equivalent) of the cutoff distances for every axis, formatted as
        [x_min, x_max, y_min, y_max, z_min, z_max]. Only consider the cutpoints which
        fall within these cutoff distances. The default is None, in which case they
        are automatically computed using `get_cutoffs`.
    verbose : bool, optional
        Provide extra information when computing the cutpoints: time the operation
        and show a progress bar. The default is `True`.

    Attributes
    ----------
    line_data : instance of pept.LineData
        The LoRs for which the cutpoints will be computed. It is required to be an
        instance of `pept.LineData`.
    max_distance : float
        The maximum distance between any two lines so that their cutpoint will be
        considered.
    cutoffs : list-like of length 6
        A list (or equivalent) of the cutoff distances for every axis, formatted as
        [x_min, x_max, y_min, y_max, z_min, z_max]. Only consider the cutpoints which
        fall within these cutoff distances.
    sample_size, overlap, number_of_lines, etc. : inherited from pept.PointData
        Extra attributes and methods are inherited from the base class `PointData`.

    Raises
    ------
    Exception
        If `line_data` is not an instance of `pept.LineData`.
    TypeError
        If `cutoffs` is not a one-dimensional array with values formatted as
        `[min_x, max_x, min_y, max_y, min_z, max_z]`.

    Example usage
    -------------
    Compute the cutpoints for a `LineData` instance:
        &gt;&gt;&gt; line_data = pept.LineData(example_data)
        &gt;&gt;&gt; cutpts = peptml.Cutpoints(line_data, 0.1)

    Compute the cutpoints for a single sample:
        &gt;&gt;&gt; sample = line_data[0]
        &gt;&gt;&gt; cutpts_sample = peptml.Cutpoints.find_cutpoints_sample(sample) # no class instantiation

    &#39;&#39;&#39;

    def __init__(self,
                 line_data,
                 max_distance,
                 cutoffs = None,
                 verbose = True):

        # Find the cutpoints when instantiated. The method
        # also initialises the instance as a `PointData` subclass.
        self.find_cutpoints(
            line_data,
            max_distance,
            cutoffs = cutoffs,
            verbose = verbose
        )


    @property
    def line_data(self):
        &#39;&#39;&#39;The LoRs for which the cutpoints are computed.

        line_data : instance of pept.LineData

        &#39;&#39;&#39;

        return self._line_data


    @line_data.setter
    def line_data(self, new_line_data):
        &#39;&#39;&#39; The LoRs for which the cutpoints are computed.

        Parameters
        ----------
        line_data : instance of pept.LineData
            The LoRs for which the cutpoints will be computed. It is required to be an
            instance of `pept.LineData`.

        Raises
        ------
        Exception
            If `line_data` is not an instance of `pept.LineData`.

        &#39;&#39;&#39;

        # Check line_data is an instance (or a subclass!) of pept.LineData
        if not isinstance(line_data, pept.LineData):
            raise Exception(&#39;[ERROR]: line_data should be an instance of pept.LineData&#39;)

        self._line_data = line_data


    @property
    def max_distance(self):
        &#39;&#39;&#39;The maximum distance between any pair of lines for which the cutpoint is considered.

        max_distance : float

        &#39;&#39;&#39;
        return self._max_distance


    @max_distance.setter
    def max_distance(self, new_max_distance):
        &#39;&#39;&#39;The maximum distance between any pair of lines for which the cutpoint is considered.

        max_distance : float
            The maximum distance between any two lines so that their cutpoint will be
            considered.

        &#39;&#39;&#39;
        self._max_distance = new_max_distance


    @property
    def cutoffs(self):
        &#39;&#39;&#39;Only consider the cutpoints which fall within these cutoff distances.

        A list (or equivalent) of the cutoff distances for every axis, formatted as
        [x_min, x_max, y_min, y_max, z_min, z_max].

        cutoffs : (6) list or equivalent

        &#39;&#39;&#39;

        return self._cutoffs


    @cutoffs.setter
    def cutoffs(self, new_cutoffs):
        &#39;&#39;&#39;Only consider the cutpoints which fall within these cutoff distances.

        A list (or equivalent) of the cutoff distances for every axis, formatted as
        [x_min, x_max, y_min, y_max, z_min, z_max].

        Parameters
        ----------
        new_cutoffs : list-like of length 6, optional
            A list (or equivalent) of the cutoff distances for every axis, formatted as
            [x_min, x_max, y_min, y_max, z_min, z_max]. Only consider the cutpoints which
            fall within these cutoff distances. The default is None, in which case they
            are automatically computed using `get_cutoffs`.

        Raises
        ------
        TypeError
            If `cutoffs` is not a one-dimensional array with values formatted as
            `[min_x, max_x, min_y, max_y, min_z, max_z]`.

        &#39;&#39;&#39;

        cutoffs = np.asarray(new_cutoffs, order = &#39;C&#39;, dtype = float)
        if cutoffs.ndim != 1 or len(cutoffs) != 6:
            raise TypeError(&#39;\n[ERROR]: new_cutoffs should be a one-dimensional array with values [min_x, max_x, min_y, max_y, min_z, max_z]\n&#39;)

        self._cutoffs = cutoffs


    @staticmethod
    def get_cutoffs(sample):
        &#39;&#39;&#39;Compute the cutoffs from a sample of LoR data.

        This is a static method, meaning it can be called without
        instantiating the `Cutpoints` class. It computes the cutoffs
        from the minimum and maximum values of the LoRs in `sample`
        in each dimension.

        Parameters
        ----------
        sample : (N, 7) numpy.ndarray
            A sample of LoRs, where each row is `[time, x1, y1, z1, x2, y2, z2]`,
            such that every line is defined by the points `[x1, y1, z1]` and
            `[x2, y2, z2]`.
        Returns
        -------
        cutoffs : (6) numpy.ndarray
            The computed cutoffs for each dimension, formatted as `[x_min, x_max,
            y_min, y_max, z_min, z_max]`.

        Raises
        ------
        TypeError
            If `sample` is not a numpy array with shape (N, 7).

        &#39;&#39;&#39;

        # Check sample has shape (N, 7)
        if sample.ndim != 2 or sample.shape[1] != 7:
            raise TypeError(&#39;\n[ERROR]: sample should have dimensions (N, 7). Received {}\n&#39;.format(sample.shape))

        # Compute cutoffs for cutpoints as the (min, max) values of the lines
        # Minimum value of the two points that define a line
        min_x = min(sample[:, 1].min(),
                    sample[:, 4].min())
        # Maximum value of the two points that define a line
        max_x = max(sample[:, 1].max(),
                    sample[:, 4].max())

        # Minimum value of the two points that define a line
        min_y = min(sample[:, 2].min(),
                    sample[:, 5].min())
        # Maximum value of the two points that define a line
        max_y = max(sample[:, 2].max(),
                    sample[:, 5].max())

        # Minimum value of the two points that define a line
        min_z = min(sample[:, 3].min(),
                    sample[:, 6].min())
        # Maximum value of the two points that define a line
        max_z = max(sample[:, 3].max(),
                    sample[:, 6].max())

        cutoffs = np.array([min_x, max_x, min_y, max_y, min_z, max_z])
        return cutoffs


    @staticmethod
    def find_cutpoints_sample(sample, max_distance, cutoffs = None):
        &#39;&#39;&#39;Find the cutpoints in a sample of LoRs.

        This is a static method, meaning it can be called without
        instantiating the `Cutpoints` class. It computes the cutpoints
        from a given `sample` that are associated with pairs of lines
        closer than `max_distance`.

        Parameters
        ----------
        sample : (N, 7) numpy.ndarray
            A sample of LoRs, where each row is `[time, x1, y1, z1, x2, y2, z2]`,
            such that every line is defined by the points `[x1, y1, z1]` and
            `[x2, y2, z2]`.
        max_distance : float
            The maximum distance between any pair of lines so that their cutpoint
            will be considered.
        cutoffs : list, optional
            The cutoffs for each dimension, formatted as `[x_min, x_max,
            y_min, y_max, z_min, z_max]`. If not defined, they are computed
            automatically by calling `get_cutoffs`. The default is `None`.

        Returns
        -------
        sample_cutpoints : (N, 4) numpy.ndarray
            The computed cutpoints for the given LoRs, where each row is
            formatted as `[time, x, y, z]` for every cutpoint.

        Raises
        ------
        TypeError
            If `sample` is not a numpy array with shape (N, 7).
        TypeError
            If `cutoffs` is not a `one-dimensional array with values [min_x,
            max_x, min_y, max_y, min_z, max_z]`

        &#39;&#39;&#39;

        # Check sample has shape (N, 7)
        if sample.ndim != 2 or sample.shape[1] != 7:
            raise TypeError(&#39;\n[ERROR]: sample should have dimensions (N, 7). Received {}\n&#39;.format(sample.shape))

        if cutoffs is None:
            cutoffs = Cutpoints.get_cutoffs(sample)
        else:
            cutoffs = np.asarray(cutoffs, order = &#39;C&#39;, dtype = float)
            if cutoffs.ndim != 1 or len(cutoffs) != 6:
                raise TypeError(&#39;\n[ERROR]: cutoffs should be a one-dimensional array with values [min_x, max_x, min_y, max_y, min_z, max_z]\n&#39;)

        sample_cutpoints = find_cutpoints_api(sample, max_distance, cutoffs)
        return sample_cutpoints


    def find_cutpoints(self,
                       line_data,
                       max_distance,
                       cutoffs = None,
                       verbose = False):
        &#39;&#39;&#39;Find the cutpoints of the samples in a `LineData` instance.

        Parameters
        ----------
        line_data : instance of pept.LineData
            The LoRs for which the cutpoints will be computed. It is required to be an
            instance of `pept.LineData`.
        max_distance : float
            The maximum distance between any two lines so that their cutpoint will be
            considered.
        cutoffs : list-like of length 6, optional
            A list (or equivalent) of the cutoff distances for every axis, formatted as
            [x_min, x_max, y_min, y_max, z_min, z_max]. Only consider the cutpoints which
            fall within these cutoff distances. The default is None, in which case they
            are automatically computed using `get_cutoffs`.
        verbose : bool, optional
            Provide extra information when computing the cutpoints: time the operation
            and show a progress bar. The default is `False`.

        Returns
        -------
        self : the PointData instance of cutpoints
            The computed cutpoints are stored in the `Cutpoints` class, as a
            subclass of `pept.PointData`.

        Raises
        ------
        Exception
            If `line_data` is not an instance of `pept.LineData`.
        TypeError
            If `cutoffs` is not a one-dimensional array with values formatted as
            `[min_x, max_x, min_y, max_y, min_z, max_z]`.

        &#39;&#39;&#39;

        if verbose:
            start = time.time()

        # Check line_data is an instance (or a subclass!) of pept.LineData
        if not isinstance(line_data, pept.LineData):
            raise Exception(&#39;[ERROR]: line_data should be an instance of pept.LineData&#39;)

        self._line_data = line_data
        self._max_distance = max_distance

        # If cutoffs were not supplied, compute them
        if cutoffs is None:
            cutoffs = self.get_cutoffs(line_data.line_data)
        # Otherwise make sure they are a C-contiguous numpy array
        else:
            cutoffs = np.asarray(cutoffs, order = &#39;C&#39;, dtype = float)
            if cutoffs.ndim != 1 or len(cutoffs) != 6:
                raise TypeError(&#39;\n[ERROR]: cutoffs should be a one-dimensional array with values [min_x, max_x, min_y, max_y, min_z, max_z]\n&#39;)

        self._cutoffs = cutoffs

        # Using joblib, collect the cutpoints from every sample in a list
        # of arrays. If verbose, show progress bar using tqdm.
        if verbose:
            cutpoints = Parallel(n_jobs = -1, prefer = &#39;threads&#39;)(delayed(self.find_cutpoints_sample)(sample, max_distance, cutoffs) for sample in tqdm(line_data))
        else:
            cutpoints = Parallel(n_jobs = -1, prefer = &#39;threads&#39;)(delayed(self.find_cutpoints_sample)(sample, max_distance, cutoffs) for sample in line_data)

        # cutpoints shape: (n, m, 4), where n is the number of samples, and
        # m is the number of cutpoints in the sample
        cutpoints = np.array(cutpoints)

        number_of_samples = len(cutpoints)
        cutpoints = np.vstack(np.array(cutpoints))
        number_of_cutpoints = len(cutpoints)

        # Average number of cutpoints per sample
        cutpoints_per_sample = int(number_of_cutpoints / number_of_samples)

        super().__init__(cutpoints,
                         sample_size = cutpoints_per_sample,
                         overlap = 0,
                         verbose = False)

        if verbose:
            end = time.time()
            print(&#34;\nFinding the cutpoints took {} seconds\n&#34;.format(end - start))

        return self




def findMeanError(truePositions, foundPositions):

    tree = cKDTree(truePositions)

    meanError = 0
    meanErrorX = 0
    meanErrorY = 0
    meanErrorZ = 0
    n = 0
    for centre in foundPositions:
        d, index = tree.query(centre, k = 1,  n_jobs = -1)
        meanError += np.linalg.norm(centre - truePositions[index])

        meanErrorX += np.abs(centre[0] - truePositions[index][0])
        meanErrorY += np.abs(centre[1] - truePositions[index][1])
        meanErrorZ += np.abs(centre[2] - truePositions[index][2])

        n += 1

    meanError /= n

    meanErrorX /= n
    meanErrorY /= n
    meanErrorZ /= n

    return [meanError, meanErrorX, meanErrorY, meanErrorZ]




class HDBSCANClusterer:
    &#39;&#39;&#39;HDBSCAN-based clustering for cutpoints from LoRs.

    This class is a wrapper around the `hdbscan` package, providing tools for
    parallel clustering of samples of cutpoints. It can return `PointData`
    classes which can be easily manipulated or visualised.

    Parameters
    ----------
        min_cluster_size : int, optional
            (Taken from hdbscan&#39;s documentation): The minimum size of clusters;
            single linkage splits that contain fewer points than this will be
            considered points “falling out” of a cluster rather than a cluster
            splitting into two new clusters. The default is 20.
        min_samples : int, optional
            (Taken from hdbscan&#39;s documentation): The number of samples in a
            neighbourhood for a point to be considered a core point. The default
            is None, being set automatically to the `min_cluster_size`.
        allow_single_cluster : bool, optional
            (Taken from hdbscan&#39;s documentation): By default HDBSCAN* will not
            produce a single cluster, setting this to True will override this and
            allow single cluster results in the case that you feel this is a valid
            result for your dataset. For PEPT, set this to True if you only have
            one tracer in the dataset. Otherwise, leave it to False, as it will
            provide higher accuracy.

    Attributes
    ----------
        min_cluster_size : int
            (Taken from hdbscan&#39;s documentation): The minimum size of clusters;
            single linkage splits that contain fewer points than this will be
            considered points “falling out” of a cluster rather than a cluster
            splitting into two new clusters. The default is 20.
        min_samples : int
            (Taken from hdbscan&#39;s documentation): The number of samples in a
            neighbourhood for a point to be considered a core point. The default
            is None, being set automatically to the `min_cluster_size`.
        allow_single_cluster : bool
            (Taken from hdbscan&#39;s documentation): By default HDBSCAN* will not
            produce a single cluster, setting this to True will override this and
            allow single cluster results in the case that you feel this is a valid
            result for your dataset. For PEPT, set this to True if you only have
            one tracer in the dataset. Otherwise, leave it to False, as it will
            provide higher accuracy.

    &#39;&#39;&#39;

    def __init__(self,
                 min_cluster_size = 20,
                 min_samples = None,
                 allow_single_cluster = False):

        if 0 &lt; min_cluster_size &lt; 2:
            print(&#34;\n[WARNING]: min_cluster_size was set to 2, as it was {} &lt; 2\n&#34;.format(min_cluster_size))
            min_cluster_size = 2

        self.clusterer = hdbscan.HDBSCAN(min_cluster_size = min_cluster_size,
                                         min_samples = min_samples,
                                         core_dist_n_jobs = -1,
                                         allow_single_cluster = allow_single_cluster)


    @property
    def min_cluster_size(self):
        return self.clusterer.min_cluster_size


    @min_cluster_size.setter
    def min_cluster_size(self, new_min_cluster_size):
        self.clusterer.min_cluster_size = new_min_cluster_size


    @property
    def min_samples(self):
        return self.clusterer.min_cluster_size


    @min_samples.setter
    def min_samples(self, new_min_samples):
        self.clusterer.min_samples = new_min_samples


    @property
    def allow_single_cluster(self):
        return self.clusterer.allow_single_cluster


    @allow_single_cluster.setter
    def allow_single_cluster(self, option):
        self.clusterer.allow_single_cluster = option


    def fit_sample(self,
                   sample,
                   store_labels = False,
                   noise = False,
                   as_array = True,
                   verbose = False):
        &#39;&#39;&#39;Fit one sample of cutpoints and return the cluster centres and
        (optionally) the labelled cutpoints.

        Parameters
        ----------
        sample : (N, M &gt;= 4) numpy.ndarray
            The sample of points that will be clustered. Every point corresponds to
            a row and is formatted as `[time, x, y, z, etc]`. Only columns `[1, 2, 3]`
            are used for clustering.
        store_labels : bool, optional
            If set to True, the clustered cutpoints are returned along with the centres
            of the clusters. Setting it to False speeds up the clustering. The default
            is False.
        noise : bool, optional
            If set to True, the clustered cutpoints also include the points classified
            as noise. Only has an effect if `store_labels` is set to True. The default
            is False.
        as_array : bool, optional
            If set to True, the centres of the clusters and the clustered cutpoints are
            returned as numpy arrays. If set to False, they are returned inside
            instances of `pept.PointData`.
        verbose : bool, optional
            Provide extra information when computing the cutpoints: time the operation
            and show a progress bar. The default is `False`.

        Returns
        -------
        centres : numpy.ndarray or pept.PointData
            The centroids of every cluster found. They are computed as the average
            of every column of `[time, x, y, z, etc]` of the clustered points. Another
            column is added to the initial data in `sample`, signifying the cluster
            size - the number of points included in the cluster. If `as_array` is
            set to True, it is a numpy array, otherwise the centres are stored
            in a pept.PointData instance.
        clustered_cutpoints : numpy.ndarray or pept.PointData
            The points in `sample` that fall in every cluster. A new column is added
            to the points in `sample` that signifies the label of cluster that the
            point was associated with: all points in cluster number 3 will have the
            number 3 as the last element in their row. The points classified as noise
            have the number -1 associated. If `as_array` is set to True, it is a numpy
            array, otherwise the clustered cutpoints are stored in a pept.PointData
            instance.

        Raises
        ------
        TypeError
            If `sample` is not a numpy array of shape (N, M), where M &gt;= 4.

        &#39;&#39;&#39;

        if verbose:
            start = time.time()

        # sample row: [time, x, y, z]
        if sample.ndim != 2 or sample.shape[1] &lt; 4:
            raise TypeError(&#39;\n[ERROR]: sample should have two dimensions (M, N), where N &gt;= 4. Received {}\n&#39;.format(sample.shape))

        # Only cluster based on [x, y, z]
        labels = self.clusterer.fit_predict(sample[:, 1:4])
        max_label = labels.max()

        centres = []
        clustered_cutpoints = []

        # the centre of a cluster is the average of the time, x, y, z columns
        # and the number of points of that cluster
        # centres row: [time, x, y, z, ..etc.., cluster_size]
        for i in range(0, max_label + 1):
            # Average time, x, y, z of cluster of label i
            centres_row = np.mean(sample[labels == i], axis = 0)
            # Append the number of points of label i =&gt; cluster_size
            centres_row = np.append(centres_row, (labels == i).sum())
            centres.append(centres_row)

        centres = np.array(centres)

        if not as_array:
            centres = pept.PointData(centres,
                                     sample_size = 0,
                                     overlap = 0,
                                     verbose = False)

        # Return all cutpoints as a list of numpy arrays for every label
        # where the last column of an array is the label
        if store_labels:
            # Create a list of numpy arrays with rows: [t, x, y, z, ..etc.., label]
            if noise:
                cutpoints = sample[labels == -1]
                cutpoints = np.insert(cutpoints, cutpoints.shape[1], -1, axis = 1)
                clustered_cutpoints.append(cutpoints)

            for i in range(0, max_label + 1):
                cutpoints = sample[labels == i]
                cutpoints = np.insert(cutpoints, cutpoints.shape[1], i, axis = 1)
                clustered_cutpoints.append(cutpoints)

            clustered_cutpoints = np.vstack(np.array(clustered_cutpoints))

            if not as_array:
                clustered_cutpoints = pept.PointData(clustered_cutpoints,
                                                     sample_size = 0,
                                                     overlap = 0,
                                                     verbose = False)

        if verbose:
            end = time.time()
            print(&#34;Fitting one sample took {} seconds&#34;.format(end - start))

        return [centres, clustered_cutpoints]


    def fit_cutpoints(self,
                      cutpoints,
                      store_labels = False,
                      noise = False,
                      verbose = True):
        &#39;&#39;&#39;Fit cutpoints (an instance of `PointData`) and return the cluster
        centres and (optionally) the labelled cutpoints.

        Parameters
        ----------
        cutpoints : an instance of `pept.PointData`
            The samples of points that will be clustered. In every sample, every point
            corresponds to a row and is formatted as `[time, x, y, z, etc]`. Only
            columns `[1, 2, 3]` are used for clustering.
        store_labels : bool, optional
            If set to True, the clustered cutpoints are returned along with the centres
            of the clusters. Setting it to False speeds up the clustering. The default
            is False.
        noise : bool, optional
            If set to True, the clustered cutpoints also include the points classified
            as noise. Only has an effect if `store_labels` is set to True. The default
            is False.
        verbose : bool, optional
            Provide extra information when computing the cutpoints: time the operation
            and show a progress bar. The default is `False`.

        Returns
        -------
        centres : pept.PointData
            The centroids of every cluster found. They are computed as the average
            of every column of `[time, x, y, z, etc]` of the clustered points. Another
            column is added to the initial data in `sample`, signifying the cluster
            size - the number of points included in the cluster.
        clustered_cutpoints : numpy.ndarray or pept.PointData
            The points in `sample` that fall in every cluster. A new column is added
            to the points in `sample` that signifies the label of cluster that the
            point was associated with: all points in cluster number 3 will have the
            number 3 as the last element in their row. The points classified as noise
            have the number -1 associated.

        Raises
        ------
        Exception
            If `cutpoints` is not an instance (or a subclass) of `pept.PointData`.

        &#39;&#39;&#39;

        if verbose:
            start = time.time()

        if not isinstance(cutpoints, pept.PointData):
            raise Exception(&#39;[ERROR]: cutpoints should be an instance of pept.PointData (or any class inheriting from it)&#39;)

        # Fit all samples in `cutpoints` in parallel using joblib
        # Collect all outputs as a list. If verbose, show progress bar with
        # tqdm
        if verbose:
            data_list = Parallel(n_jobs = -1)(delayed(self.fit_sample)(sample,
                                                store_labels = store_labels,
                                                noise = noise,
                                                as_array = True) for sample in tqdm(cutpoints))
        else:
            data_list = Parallel(n_jobs = -1)(delayed(self.fit_sample)(sample,
                                                store_labels = store_labels,
                                                noise = noise,
                                                as_array = True) for sample in cutpoints)

        # Access joblib.Parallel output as list comprehensions
        centres = np.array([row[0] for row in data_list if len(row[0]) != 0])
        if len(centres) != 0:
            centres = pept.PointData(np.vstack(centres),
                                     sample_size = 0,
                                     overlap = 0,
                                     verbose = False)

        if store_labels:
            clustered_cutpoints = np.array([row[1] for row in data_list if len(row[1]) != 0])
            clustered_cutpoints = pept.PointData(np.vstack(np.array(clustered_cutpoints)),
                                                 sample_size = 0,
                                                 overlap = 0,
                                                 verbose = False)

        if verbose:
            end = time.time()
            print(&#34;\nFitting cutpoints took {} seconds\n&#34;.format(end - start))

        if store_labels:
            return [centres, clustered_cutpoints]
        else:
            return [centres, []]




class TrajectorySeparation:

    def __init__(self, centres, pointsToCheck = 25, maxDistance = 20, maxClusterDiff = 500):
        # centres row: [time, x, y, z, clusterSize]
        # Make sure the trajectory is memory-contiguous for efficient
        # KDTree partitioning
        self.centres = np.ascontiguousarray(centres)
        self.pointsToCheck = pointsToCheck
        self.maxDistance = maxDistance
        self.maxClusterDiff = maxClusterDiff

        # For every point in centres, save a set of the trajectory
        # indices of the trajectories that they are part of
        #   eg. centres[2] is part of trajectories 0 and 1 =&gt;
        #   trajectoryIndices[2] = {0, 1}
        # Initialise a vector of empty sets of size len(centres)
        self.trajectoryIndices = np.array([ set() for i in range(len(self.centres)) ])

        # For every trajectory found, save a list of the indices of
        # the centres that are part of that trajectory
        #   eg. trajectory 1 is comprised of centres 3, 5 and 8 =&gt;
        #   centresIndices[1] = [3, 5, 8]
        self.centresIndices = [[]]

        # Maximum trajectory index
        self.maxIndex = 0


    def findTrajectories(self):

        for i, currentPoint in enumerate(self.centres):

            if i == 0:
                # Add the first point to trajectory 0
                self.trajectoryIndices[0].add(self.maxIndex)
                self.centresIndices[self.maxIndex].append(0)
                self.maxIndex += 1
                continue

            # Search for the closest previous pointsToCheck points
            # within a given maxDistance
            startIndex = i - self.pointsToCheck
            endIndex = i

            if startIndex &lt; 0:
                startIndex = 0

            # Construct a KDTree from the x, y, z (1:4) of the
            # selected points. Get the indices for all the points within
            # maxDistance of the currentPoint
            tree = cKDTree(self.centres[startIndex:endIndex, 1:4])
            closestIndices = tree.query_ball_point(currentPoint[1:4], self.maxDistance, n_jobs=-1)
            closestIndices = np.array(closestIndices) + startIndex

            # If no point was found, it is a new trajectory. Continue
            if len(closestIndices) == 0:
                self.trajectoryIndices[i].add(self.maxIndex)
                self.centresIndices.append([i])
                self.maxIndex += 1
                continue

            # For every close point found, search for all the trajectory indices
            #   - If all trajectory indices sets are equal and of a single value
            #   then currentPoint is part of the same trajectory
            #   - If all trajectory indices sets are equal, but of more values,
            #   then currentPoint diverged from an intersection of trajectories
            #   and is part of a single trajectory =&gt; separate it
            #
            #   - If every pair of trajectory indices sets is not disjoint, then
            #   currentPoint is only one of them
            #   - If there exists a pair of trajectory indices sets that is
            #   disjoint, then currentPoint is part of all of them

            # Select the trajectories of all the points that were found
            # to be the closest
            closestTrajectories = self.trajectoryIndices[closestIndices]
            #print(&#34;closestTrajectories:&#34;)
            #print(closestTrajectories)

            # If all the closest points are part of the same trajectory
            # (just one!), then the currentPoint is part of it too
            if (np.all(closestTrajectories == closestTrajectories[0]) and
                len(closestTrajectories[0]) == 1):

                self.trajectoryIndices[i] = closestTrajectories[0]
                self.centresIndices[ next(iter(closestTrajectories[0])) ].append(i)
                continue

            # Otherwise, check the points based on their cluster size
            else:
                # Create a list of all the trajectories that were found to
                # intersect
                #print(&#39;\nIntersection:&#39;)
                closestTrajIndices = list( set().union(*closestTrajectories) )

                #print(&#34;ClosestTrajIndices:&#34;)
                #print(closestTrajIndices)

                # For each close trajectory, calculate the mean cluster size
                # of the last lastPoints points
                lastPoints = 50

                # Keep track of the mean cluster size that is the closest to
                # the currentPoint&#39;s clusterSize
                currentClusterSize = currentPoint[4]
                #print(&#34;currentClusterSize = {}&#34;.format(currentClusterSize))
                closestTrajIndex = -1
                clusterSizeDiff = self.maxClusterDiff

                for trajIndex in closestTrajIndices:
                    #print(&#34;trajIndex = {}&#34;.format(trajIndex))

                    trajCentres = self.centres[ self.centresIndices[trajIndex] ]
                    #print(&#34;trajCentres:&#34;)
                    #print(trajCentres)
                    meanClusterSize = trajCentres[-lastPoints:][:, 4].mean()
                    #print(&#34;meanClusterSize = {}&#34;.format(meanClusterSize))
                    #print(&#34;clusterSizeDiff = {}&#34;.format(clusterSizeDiff))
                    #print(&#34;abs diff = {}&#34;.format(np.abs( currentClusterSize - meanClusterSize )))
                    if np.abs( currentClusterSize - meanClusterSize ) &lt; clusterSizeDiff:
                        closestTrajIndex = trajIndex
                        clusterSizeDiff = np.abs( currentClusterSize - meanClusterSize )

                if closestTrajIndex == -1:
                    #self.trajectoryIndices[i] = set(closestTrajIndices)
                    #for trajIndex in closestTrajIndices:
                    #    self.centresIndices[trajIndex].append(i)

                    print(&#34;\n**** -1 ****\n&#34;)
                    break
                else:
                    #print(&#34;ClosestTrajIndex found = {}&#34;.format(closestTrajIndex))
                    self.trajectoryIndices[i] = set([closestTrajIndex])
                    self.centresIndices[closestTrajIndex].append(i)




            &#39;&#39;&#39;
            # If the current point is not part of any trajectory, assign it
            # the maxIndex and increment it
            if len(self.trajectoryIndices[i]) == 0:
                self.trajectoryIndices[i].append(self.maxIndex)
                self.maxIndex += 1

            print(self.trajectoryIndices[i])
            print(self.maxIndex)

            # Construct a KDTree from the numberOfPoints in front of
            # the current point
            tree = cKDTree(self.trajectory[(i + 1):(i + self.numberOfPoints + 2)][1:4])

            # For every trajectory that the current point is part of,
            # find the closest points in front of it
            numberOfIntersections = len(self.trajectoryIndices[i])
            dist, nextPointsIndices = tree.query(currentPoint, k=numberOfIntersections, distance_upper_bound=self.maxDistance, n_jobs=-1)

            print(nextPointsIndices)

            # If the current point is part of more trajectories,
            # an intersection happened. Call subroutine to part
            # the trajectories
            if numberOfIntersections &gt; 1:
                for j in range(0, len(self.trajectoryIndices[i])):
                    trajIndex = self.trajectoryIndices[i][j]
                    self.trajectoryIndices[i + 1 + nextPointsIndices[j]].append(trajIndex)

            else:
                self.trajectoryIndices[i + 1 + nextPointsIndices].append(self.trajectoryIndices[i][0])

            print(self.trajectoryIndices)
            &#39;&#39;&#39;


    def getTrajectories(self):

        self.individualTrajectories = []
        for trajCentres in self.centresIndices:
            self.individualTrajectories.append(self.centres[trajCentres])

        self.individualTrajectories = np.array(self.individualTrajectories)
        return self.individualTrajectories

        &#39;&#39;&#39;
        self.individualTrajectories = [ [] for i in range(0, self.maxIndex + 1) ]
        for i in range(0, len(self.trajectoryIndices)):
            for trajIndex in self.trajectoryIndices[i]:
                self.individualTrajectories[trajIndex].append(self.centres[i])

        self.individualTrajectories = np.array(self.individualTrajectories)
        for i in range(len(self.individualTrajectories)):
            if len(self.individualTrajectories[i]) &gt; 0:
                self.individualTrajectories[i] = np.vstack(self.individualTrajectories[i])
        return self.individualTrajectories
        &#39;&#39;&#39;


    def plotTrajectoriesAltAxes(self, ax):
        trajectories = self.getTrajectories()
        for traj in trajectories:
            if len(traj) &gt; 0:
                ax.scatter(traj[:, 3], traj[:, 1], traj[:, 2], marker=&#39;D&#39;, s=10)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="pept.tracking.peptml.peptml.findMeanError"><code class="name flex">
<span>def <span class="ident">findMeanError</span></span>(<span>truePositions, foundPositions)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def findMeanError(truePositions, foundPositions):

    tree = cKDTree(truePositions)

    meanError = 0
    meanErrorX = 0
    meanErrorY = 0
    meanErrorZ = 0
    n = 0
    for centre in foundPositions:
        d, index = tree.query(centre, k = 1,  n_jobs = -1)
        meanError += np.linalg.norm(centre - truePositions[index])

        meanErrorX += np.abs(centre[0] - truePositions[index][0])
        meanErrorY += np.abs(centre[1] - truePositions[index][1])
        meanErrorZ += np.abs(centre[2] - truePositions[index][2])

        n += 1

    meanError /= n

    meanErrorX /= n
    meanErrorY /= n
    meanErrorZ /= n

    return [meanError, meanErrorX, meanErrorY, meanErrorZ]</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="pept.tracking.peptml.peptml.Cutpoints"><code class="flex name class">
<span>class <span class="ident">Cutpoints</span></span>
<span>(</span><span>line_data, max_distance, cutoffs=None, verbose=True)</span>
</code></dt>
<dd>
<section class="desc"><p>A class that transforms LoRs into <em>cutpoints</em> for clustering.</p>
<p>The <a title="pept.tracking.peptml.peptml.Cutpoints" href="#pept.tracking.peptml.peptml.Cutpoints"><code>Cutpoints</code></a> class transforms LoRs (individual numpy arrays or full <code>LineData</code>)
into cutpoints (individual numpy arrays or full <code>PointData</code>) that can then be passed
to <a title="pept.tracking.peptml.peptml.HDBSCANClusterer" href="#pept.tracking.peptml.peptml.HDBSCANClusterer"><code>HDBSCANClusterer</code></a>.</p>
<p>Under typical usage, the class is instantiated with the LoR data (required to be
an instance of <code>LineData</code>) and transforms it into an instance of <code>PointData</code> that
stores the found cutpoints.</p>
<p>For more control over the operations, the class also provides a static method (i.e.
it can be used without instantiating the class) <code>find_cutpoints_sample</code> that
receives a generic numpy array of LoRs (one 'sample') and returns a numpy array
of cutpoints.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>line_data</code></strong> :&ensp;<code>instance</code> of <a title="pept.LineData" href="../../index.html#pept.LineData"><code>LineData</code></a></dt>
<dd>The LoRs for which the cutpoints will be computed. It is required to be an
instance of <a title="pept.LineData" href="../../index.html#pept.LineData"><code>LineData</code></a>.</dd>
<dt><strong><code>max_distance</code></strong> :&ensp;<code>float</code></dt>
<dd>The maximum distance between any two lines so that their cutpoint will be
considered.</dd>
<dt><strong><code>cutoffs</code></strong> :&ensp;<code>list</code>-<code>like</code> of <code>length</code> <code>6</code>, optional</dt>
<dd>A list (or equivalent) of the cutoff distances for every axis, formatted as
[x_min, x_max, y_min, y_max, z_min, z_max]. Only consider the cutpoints which
fall within these cutoff distances. The default is None, in which case they
are automatically computed using <code>get_cutoffs</code>.</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Provide extra information when computing the cutpoints: time the operation
and show a progress bar. The default is <code>True</code>.</dd>
</dl>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>line_data</code></strong> :&ensp;<code>instance</code> of <a title="pept.LineData" href="../../index.html#pept.LineData"><code>LineData</code></a></dt>
<dd>The LoRs for which the cutpoints will be computed. It is required to be an
instance of <a title="pept.LineData" href="../../index.html#pept.LineData"><code>LineData</code></a>.</dd>
<dt><strong><code>max_distance</code></strong> :&ensp;<code>float</code></dt>
<dd>The maximum distance between any two lines so that their cutpoint will be
considered.</dd>
<dt><strong><code>cutoffs</code></strong> :&ensp;<code>list</code>-<code>like</code> of <code>length</code> <code>6</code></dt>
<dd>A list (or equivalent) of the cutoff distances for every axis, formatted as
[x_min, x_max, y_min, y_max, z_min, z_max]. Only consider the cutpoints which
fall within these cutoff distances.</dd>
</dl>
<p>sample_size, overlap, number_of_lines, etc. : inherited from pept.PointData
Extra attributes and methods are inherited from the base class <code>PointData</code>.</p>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>Exception</code></dt>
<dd>If <code>line_data</code> is not an instance of <a title="pept.LineData" href="../../index.html#pept.LineData"><code>LineData</code></a>.</dd>
<dt><code>TypeError</code></dt>
<dd>If <code>cutoffs</code> is not a one-dimensional array with values formatted as
<code>[min_x, max_x, min_y, max_y, min_z, max_z]</code>.</dd>
</dl>
<h2 id="example-usage">Example usage</h2>
<p>Compute the cutpoints for a <code>LineData</code> instance:
&gt;&gt;&gt; line_data = pept.LineData(example_data)
&gt;&gt;&gt; cutpts = peptml.Cutpoints(line_data, 0.1)</p>
<p>Compute the cutpoints for a single sample:
&gt;&gt;&gt; sample = line_data[0]
&gt;&gt;&gt; cutpts_sample = peptml.Cutpoints.find_cutpoints_sample(sample) # no class instantiation</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">class Cutpoints(pept.PointData):
    &#39;&#39;&#39;A class that transforms LoRs into *cutpoints* for clustering.

    The `Cutpoints` class transforms LoRs (individual numpy arrays or full `LineData`)
    into cutpoints (individual numpy arrays or full `PointData`) that can then be passed
    to `HDBSCANClusterer`.

    Under typical usage, the class is instantiated with the LoR data (required to be
    an instance of `LineData`) and transforms it into an instance of `PointData` that
    stores the found cutpoints.

    For more control over the operations, the class also provides a static method (i.e.
    it can be used without instantiating the class) `find_cutpoints_sample` that
    receives a generic numpy array of LoRs (one &#39;sample&#39;) and returns a numpy array
    of cutpoints.

    Parameters
    ----------
    line_data : instance of pept.LineData
        The LoRs for which the cutpoints will be computed. It is required to be an
        instance of `pept.LineData`.
    max_distance : float
        The maximum distance between any two lines so that their cutpoint will be
        considered.
    cutoffs : list-like of length 6, optional
        A list (or equivalent) of the cutoff distances for every axis, formatted as
        [x_min, x_max, y_min, y_max, z_min, z_max]. Only consider the cutpoints which
        fall within these cutoff distances. The default is None, in which case they
        are automatically computed using `get_cutoffs`.
    verbose : bool, optional
        Provide extra information when computing the cutpoints: time the operation
        and show a progress bar. The default is `True`.

    Attributes
    ----------
    line_data : instance of pept.LineData
        The LoRs for which the cutpoints will be computed. It is required to be an
        instance of `pept.LineData`.
    max_distance : float
        The maximum distance between any two lines so that their cutpoint will be
        considered.
    cutoffs : list-like of length 6
        A list (or equivalent) of the cutoff distances for every axis, formatted as
        [x_min, x_max, y_min, y_max, z_min, z_max]. Only consider the cutpoints which
        fall within these cutoff distances.
    sample_size, overlap, number_of_lines, etc. : inherited from pept.PointData
        Extra attributes and methods are inherited from the base class `PointData`.

    Raises
    ------
    Exception
        If `line_data` is not an instance of `pept.LineData`.
    TypeError
        If `cutoffs` is not a one-dimensional array with values formatted as
        `[min_x, max_x, min_y, max_y, min_z, max_z]`.

    Example usage
    -------------
    Compute the cutpoints for a `LineData` instance:
        &gt;&gt;&gt; line_data = pept.LineData(example_data)
        &gt;&gt;&gt; cutpts = peptml.Cutpoints(line_data, 0.1)

    Compute the cutpoints for a single sample:
        &gt;&gt;&gt; sample = line_data[0]
        &gt;&gt;&gt; cutpts_sample = peptml.Cutpoints.find_cutpoints_sample(sample) # no class instantiation

    &#39;&#39;&#39;

    def __init__(self,
                 line_data,
                 max_distance,
                 cutoffs = None,
                 verbose = True):

        # Find the cutpoints when instantiated. The method
        # also initialises the instance as a `PointData` subclass.
        self.find_cutpoints(
            line_data,
            max_distance,
            cutoffs = cutoffs,
            verbose = verbose
        )


    @property
    def line_data(self):
        &#39;&#39;&#39;The LoRs for which the cutpoints are computed.

        line_data : instance of pept.LineData

        &#39;&#39;&#39;

        return self._line_data


    @line_data.setter
    def line_data(self, new_line_data):
        &#39;&#39;&#39; The LoRs for which the cutpoints are computed.

        Parameters
        ----------
        line_data : instance of pept.LineData
            The LoRs for which the cutpoints will be computed. It is required to be an
            instance of `pept.LineData`.

        Raises
        ------
        Exception
            If `line_data` is not an instance of `pept.LineData`.

        &#39;&#39;&#39;

        # Check line_data is an instance (or a subclass!) of pept.LineData
        if not isinstance(line_data, pept.LineData):
            raise Exception(&#39;[ERROR]: line_data should be an instance of pept.LineData&#39;)

        self._line_data = line_data


    @property
    def max_distance(self):
        &#39;&#39;&#39;The maximum distance between any pair of lines for which the cutpoint is considered.

        max_distance : float

        &#39;&#39;&#39;
        return self._max_distance


    @max_distance.setter
    def max_distance(self, new_max_distance):
        &#39;&#39;&#39;The maximum distance between any pair of lines for which the cutpoint is considered.

        max_distance : float
            The maximum distance between any two lines so that their cutpoint will be
            considered.

        &#39;&#39;&#39;
        self._max_distance = new_max_distance


    @property
    def cutoffs(self):
        &#39;&#39;&#39;Only consider the cutpoints which fall within these cutoff distances.

        A list (or equivalent) of the cutoff distances for every axis, formatted as
        [x_min, x_max, y_min, y_max, z_min, z_max].

        cutoffs : (6) list or equivalent

        &#39;&#39;&#39;

        return self._cutoffs


    @cutoffs.setter
    def cutoffs(self, new_cutoffs):
        &#39;&#39;&#39;Only consider the cutpoints which fall within these cutoff distances.

        A list (or equivalent) of the cutoff distances for every axis, formatted as
        [x_min, x_max, y_min, y_max, z_min, z_max].

        Parameters
        ----------
        new_cutoffs : list-like of length 6, optional
            A list (or equivalent) of the cutoff distances for every axis, formatted as
            [x_min, x_max, y_min, y_max, z_min, z_max]. Only consider the cutpoints which
            fall within these cutoff distances. The default is None, in which case they
            are automatically computed using `get_cutoffs`.

        Raises
        ------
        TypeError
            If `cutoffs` is not a one-dimensional array with values formatted as
            `[min_x, max_x, min_y, max_y, min_z, max_z]`.

        &#39;&#39;&#39;

        cutoffs = np.asarray(new_cutoffs, order = &#39;C&#39;, dtype = float)
        if cutoffs.ndim != 1 or len(cutoffs) != 6:
            raise TypeError(&#39;\n[ERROR]: new_cutoffs should be a one-dimensional array with values [min_x, max_x, min_y, max_y, min_z, max_z]\n&#39;)

        self._cutoffs = cutoffs


    @staticmethod
    def get_cutoffs(sample):
        &#39;&#39;&#39;Compute the cutoffs from a sample of LoR data.

        This is a static method, meaning it can be called without
        instantiating the `Cutpoints` class. It computes the cutoffs
        from the minimum and maximum values of the LoRs in `sample`
        in each dimension.

        Parameters
        ----------
        sample : (N, 7) numpy.ndarray
            A sample of LoRs, where each row is `[time, x1, y1, z1, x2, y2, z2]`,
            such that every line is defined by the points `[x1, y1, z1]` and
            `[x2, y2, z2]`.
        Returns
        -------
        cutoffs : (6) numpy.ndarray
            The computed cutoffs for each dimension, formatted as `[x_min, x_max,
            y_min, y_max, z_min, z_max]`.

        Raises
        ------
        TypeError
            If `sample` is not a numpy array with shape (N, 7).

        &#39;&#39;&#39;

        # Check sample has shape (N, 7)
        if sample.ndim != 2 or sample.shape[1] != 7:
            raise TypeError(&#39;\n[ERROR]: sample should have dimensions (N, 7). Received {}\n&#39;.format(sample.shape))

        # Compute cutoffs for cutpoints as the (min, max) values of the lines
        # Minimum value of the two points that define a line
        min_x = min(sample[:, 1].min(),
                    sample[:, 4].min())
        # Maximum value of the two points that define a line
        max_x = max(sample[:, 1].max(),
                    sample[:, 4].max())

        # Minimum value of the two points that define a line
        min_y = min(sample[:, 2].min(),
                    sample[:, 5].min())
        # Maximum value of the two points that define a line
        max_y = max(sample[:, 2].max(),
                    sample[:, 5].max())

        # Minimum value of the two points that define a line
        min_z = min(sample[:, 3].min(),
                    sample[:, 6].min())
        # Maximum value of the two points that define a line
        max_z = max(sample[:, 3].max(),
                    sample[:, 6].max())

        cutoffs = np.array([min_x, max_x, min_y, max_y, min_z, max_z])
        return cutoffs


    @staticmethod
    def find_cutpoints_sample(sample, max_distance, cutoffs = None):
        &#39;&#39;&#39;Find the cutpoints in a sample of LoRs.

        This is a static method, meaning it can be called without
        instantiating the `Cutpoints` class. It computes the cutpoints
        from a given `sample` that are associated with pairs of lines
        closer than `max_distance`.

        Parameters
        ----------
        sample : (N, 7) numpy.ndarray
            A sample of LoRs, where each row is `[time, x1, y1, z1, x2, y2, z2]`,
            such that every line is defined by the points `[x1, y1, z1]` and
            `[x2, y2, z2]`.
        max_distance : float
            The maximum distance between any pair of lines so that their cutpoint
            will be considered.
        cutoffs : list, optional
            The cutoffs for each dimension, formatted as `[x_min, x_max,
            y_min, y_max, z_min, z_max]`. If not defined, they are computed
            automatically by calling `get_cutoffs`. The default is `None`.

        Returns
        -------
        sample_cutpoints : (N, 4) numpy.ndarray
            The computed cutpoints for the given LoRs, where each row is
            formatted as `[time, x, y, z]` for every cutpoint.

        Raises
        ------
        TypeError
            If `sample` is not a numpy array with shape (N, 7).
        TypeError
            If `cutoffs` is not a `one-dimensional array with values [min_x,
            max_x, min_y, max_y, min_z, max_z]`

        &#39;&#39;&#39;

        # Check sample has shape (N, 7)
        if sample.ndim != 2 or sample.shape[1] != 7:
            raise TypeError(&#39;\n[ERROR]: sample should have dimensions (N, 7). Received {}\n&#39;.format(sample.shape))

        if cutoffs is None:
            cutoffs = Cutpoints.get_cutoffs(sample)
        else:
            cutoffs = np.asarray(cutoffs, order = &#39;C&#39;, dtype = float)
            if cutoffs.ndim != 1 or len(cutoffs) != 6:
                raise TypeError(&#39;\n[ERROR]: cutoffs should be a one-dimensional array with values [min_x, max_x, min_y, max_y, min_z, max_z]\n&#39;)

        sample_cutpoints = find_cutpoints_api(sample, max_distance, cutoffs)
        return sample_cutpoints


    def find_cutpoints(self,
                       line_data,
                       max_distance,
                       cutoffs = None,
                       verbose = False):
        &#39;&#39;&#39;Find the cutpoints of the samples in a `LineData` instance.

        Parameters
        ----------
        line_data : instance of pept.LineData
            The LoRs for which the cutpoints will be computed. It is required to be an
            instance of `pept.LineData`.
        max_distance : float
            The maximum distance between any two lines so that their cutpoint will be
            considered.
        cutoffs : list-like of length 6, optional
            A list (or equivalent) of the cutoff distances for every axis, formatted as
            [x_min, x_max, y_min, y_max, z_min, z_max]. Only consider the cutpoints which
            fall within these cutoff distances. The default is None, in which case they
            are automatically computed using `get_cutoffs`.
        verbose : bool, optional
            Provide extra information when computing the cutpoints: time the operation
            and show a progress bar. The default is `False`.

        Returns
        -------
        self : the PointData instance of cutpoints
            The computed cutpoints are stored in the `Cutpoints` class, as a
            subclass of `pept.PointData`.

        Raises
        ------
        Exception
            If `line_data` is not an instance of `pept.LineData`.
        TypeError
            If `cutoffs` is not a one-dimensional array with values formatted as
            `[min_x, max_x, min_y, max_y, min_z, max_z]`.

        &#39;&#39;&#39;

        if verbose:
            start = time.time()

        # Check line_data is an instance (or a subclass!) of pept.LineData
        if not isinstance(line_data, pept.LineData):
            raise Exception(&#39;[ERROR]: line_data should be an instance of pept.LineData&#39;)

        self._line_data = line_data
        self._max_distance = max_distance

        # If cutoffs were not supplied, compute them
        if cutoffs is None:
            cutoffs = self.get_cutoffs(line_data.line_data)
        # Otherwise make sure they are a C-contiguous numpy array
        else:
            cutoffs = np.asarray(cutoffs, order = &#39;C&#39;, dtype = float)
            if cutoffs.ndim != 1 or len(cutoffs) != 6:
                raise TypeError(&#39;\n[ERROR]: cutoffs should be a one-dimensional array with values [min_x, max_x, min_y, max_y, min_z, max_z]\n&#39;)

        self._cutoffs = cutoffs

        # Using joblib, collect the cutpoints from every sample in a list
        # of arrays. If verbose, show progress bar using tqdm.
        if verbose:
            cutpoints = Parallel(n_jobs = -1, prefer = &#39;threads&#39;)(delayed(self.find_cutpoints_sample)(sample, max_distance, cutoffs) for sample in tqdm(line_data))
        else:
            cutpoints = Parallel(n_jobs = -1, prefer = &#39;threads&#39;)(delayed(self.find_cutpoints_sample)(sample, max_distance, cutoffs) for sample in line_data)

        # cutpoints shape: (n, m, 4), where n is the number of samples, and
        # m is the number of cutpoints in the sample
        cutpoints = np.array(cutpoints)

        number_of_samples = len(cutpoints)
        cutpoints = np.vstack(np.array(cutpoints))
        number_of_cutpoints = len(cutpoints)

        # Average number of cutpoints per sample
        cutpoints_per_sample = int(number_of_cutpoints / number_of_samples)

        super().__init__(cutpoints,
                         sample_size = cutpoints_per_sample,
                         overlap = 0,
                         verbose = False)

        if verbose:
            end = time.time()
            print(&#34;\nFinding the cutpoints took {} seconds\n&#34;.format(end - start))

        return self</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="pept.base.point_data.PointData" href="../../base/point_data.html#pept.base.point_data.PointData">PointData</a></li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="pept.tracking.peptml.peptml.Cutpoints.find_cutpoints_sample"><code class="name flex">
<span>def <span class="ident">find_cutpoints_sample</span></span>(<span>sample, max_distance, cutoffs=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Find the cutpoints in a sample of LoRs.</p>
<p>This is a static method, meaning it can be called without
instantiating the <a title="pept.tracking.peptml.peptml.Cutpoints" href="#pept.tracking.peptml.peptml.Cutpoints"><code>Cutpoints</code></a> class. It computes the cutpoints
from a given <code>sample</code> that are associated with pairs of lines
closer than <code>max_distance</code>.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>sample</code></strong> :&ensp;(<code>N</code>, <code>7</code>) <code>numpy.ndarray</code></dt>
<dd>A sample of LoRs, where each row is <code>[time, x1, y1, z1, x2, y2, z2]</code>,
such that every line is defined by the points <code>[x1, y1, z1]</code> and
<code>[x2, y2, z2]</code>.</dd>
<dt><strong><code>max_distance</code></strong> :&ensp;<code>float</code></dt>
<dd>The maximum distance between any pair of lines so that their cutpoint
will be considered.</dd>
<dt><strong><code>cutoffs</code></strong> :&ensp;<code>list</code>, optional</dt>
<dd>The cutoffs for each dimension, formatted as <code>[x_min, x_max,
y_min, y_max, z_min, z_max]</code>. If not defined, they are computed
automatically by calling <code>get_cutoffs</code>. The default is <code>None</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>sample_cutpoints</code></strong> :&ensp;(<code>N</code>, <code>4</code>) <code>numpy.ndarray</code></dt>
<dd>The computed cutpoints for the given LoRs, where each row is
formatted as <code>[time, x, y, z]</code> for every cutpoint.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>TypeError</code></dt>
<dd>If <code>sample</code> is not a numpy array with shape (N, 7).</dd>
<dt><code>TypeError</code></dt>
<dd>If <code>cutoffs</code> is not a <code>one-dimensional array with values [min_x,
max_x, min_y, max_y, min_z, max_z]</code></dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">@staticmethod
def find_cutpoints_sample(sample, max_distance, cutoffs = None):
    &#39;&#39;&#39;Find the cutpoints in a sample of LoRs.

    This is a static method, meaning it can be called without
    instantiating the `Cutpoints` class. It computes the cutpoints
    from a given `sample` that are associated with pairs of lines
    closer than `max_distance`.

    Parameters
    ----------
    sample : (N, 7) numpy.ndarray
        A sample of LoRs, where each row is `[time, x1, y1, z1, x2, y2, z2]`,
        such that every line is defined by the points `[x1, y1, z1]` and
        `[x2, y2, z2]`.
    max_distance : float
        The maximum distance between any pair of lines so that their cutpoint
        will be considered.
    cutoffs : list, optional
        The cutoffs for each dimension, formatted as `[x_min, x_max,
        y_min, y_max, z_min, z_max]`. If not defined, they are computed
        automatically by calling `get_cutoffs`. The default is `None`.

    Returns
    -------
    sample_cutpoints : (N, 4) numpy.ndarray
        The computed cutpoints for the given LoRs, where each row is
        formatted as `[time, x, y, z]` for every cutpoint.

    Raises
    ------
    TypeError
        If `sample` is not a numpy array with shape (N, 7).
    TypeError
        If `cutoffs` is not a `one-dimensional array with values [min_x,
        max_x, min_y, max_y, min_z, max_z]`

    &#39;&#39;&#39;

    # Check sample has shape (N, 7)
    if sample.ndim != 2 or sample.shape[1] != 7:
        raise TypeError(&#39;\n[ERROR]: sample should have dimensions (N, 7). Received {}\n&#39;.format(sample.shape))

    if cutoffs is None:
        cutoffs = Cutpoints.get_cutoffs(sample)
    else:
        cutoffs = np.asarray(cutoffs, order = &#39;C&#39;, dtype = float)
        if cutoffs.ndim != 1 or len(cutoffs) != 6:
            raise TypeError(&#39;\n[ERROR]: cutoffs should be a one-dimensional array with values [min_x, max_x, min_y, max_y, min_z, max_z]\n&#39;)

    sample_cutpoints = find_cutpoints_api(sample, max_distance, cutoffs)
    return sample_cutpoints</code></pre>
</details>
</dd>
<dt id="pept.tracking.peptml.peptml.Cutpoints.get_cutoffs"><code class="name flex">
<span>def <span class="ident">get_cutoffs</span></span>(<span>sample)</span>
</code></dt>
<dd>
<section class="desc"><p>Compute the cutoffs from a sample of LoR data.</p>
<p>This is a static method, meaning it can be called without
instantiating the <a title="pept.tracking.peptml.peptml.Cutpoints" href="#pept.tracking.peptml.peptml.Cutpoints"><code>Cutpoints</code></a> class. It computes the cutoffs
from the minimum and maximum values of the LoRs in <code>sample</code>
in each dimension.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>sample</code></strong> :&ensp;(<code>N</code>, <code>7</code>) <code>numpy.ndarray</code></dt>
<dd>A sample of LoRs, where each row is <code>[time, x1, y1, z1, x2, y2, z2]</code>,
such that every line is defined by the points <code>[x1, y1, z1]</code> and
<code>[x2, y2, z2]</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>cutoffs</code></strong> :&ensp;(<code>6</code>) <code>numpy.ndarray</code></dt>
<dd>The computed cutoffs for each dimension, formatted as <code>[x_min, x_max,
y_min, y_max, z_min, z_max]</code>.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>TypeError</code></dt>
<dd>If <code>sample</code> is not a numpy array with shape (N, 7).</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">@staticmethod
def get_cutoffs(sample):
    &#39;&#39;&#39;Compute the cutoffs from a sample of LoR data.

    This is a static method, meaning it can be called without
    instantiating the `Cutpoints` class. It computes the cutoffs
    from the minimum and maximum values of the LoRs in `sample`
    in each dimension.

    Parameters
    ----------
    sample : (N, 7) numpy.ndarray
        A sample of LoRs, where each row is `[time, x1, y1, z1, x2, y2, z2]`,
        such that every line is defined by the points `[x1, y1, z1]` and
        `[x2, y2, z2]`.
    Returns
    -------
    cutoffs : (6) numpy.ndarray
        The computed cutoffs for each dimension, formatted as `[x_min, x_max,
        y_min, y_max, z_min, z_max]`.

    Raises
    ------
    TypeError
        If `sample` is not a numpy array with shape (N, 7).

    &#39;&#39;&#39;

    # Check sample has shape (N, 7)
    if sample.ndim != 2 or sample.shape[1] != 7:
        raise TypeError(&#39;\n[ERROR]: sample should have dimensions (N, 7). Received {}\n&#39;.format(sample.shape))

    # Compute cutoffs for cutpoints as the (min, max) values of the lines
    # Minimum value of the two points that define a line
    min_x = min(sample[:, 1].min(),
                sample[:, 4].min())
    # Maximum value of the two points that define a line
    max_x = max(sample[:, 1].max(),
                sample[:, 4].max())

    # Minimum value of the two points that define a line
    min_y = min(sample[:, 2].min(),
                sample[:, 5].min())
    # Maximum value of the two points that define a line
    max_y = max(sample[:, 2].max(),
                sample[:, 5].max())

    # Minimum value of the two points that define a line
    min_z = min(sample[:, 3].min(),
                sample[:, 6].min())
    # Maximum value of the two points that define a line
    max_z = max(sample[:, 3].max(),
                sample[:, 6].max())

    cutoffs = np.array([min_x, max_x, min_y, max_y, min_z, max_z])
    return cutoffs</code></pre>
</details>
</dd>
</dl>
<h3>Instance variables</h3>
<dl>
<dt id="pept.tracking.peptml.peptml.Cutpoints.cutoffs"><code class="name">var <span class="ident">cutoffs</span></code></dt>
<dd>
<section class="desc"><p>Only consider the cutpoints which fall within these cutoff distances.</p>
<p>A list (or equivalent) of the cutoff distances for every axis, formatted as
[x_min, x_max, y_min, y_max, z_min, z_max].</p>
<p>cutoffs : (6) list or equivalent</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">@property
def cutoffs(self):
    &#39;&#39;&#39;Only consider the cutpoints which fall within these cutoff distances.

    A list (or equivalent) of the cutoff distances for every axis, formatted as
    [x_min, x_max, y_min, y_max, z_min, z_max].

    cutoffs : (6) list or equivalent

    &#39;&#39;&#39;

    return self._cutoffs</code></pre>
</details>
</dd>
<dt id="pept.tracking.peptml.peptml.Cutpoints.line_data"><code class="name">var <span class="ident">line_data</span></code></dt>
<dd>
<section class="desc"><p>The LoRs for which the cutpoints are computed.</p>
<p>line_data : instance of pept.LineData</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">@property
def line_data(self):
    &#39;&#39;&#39;The LoRs for which the cutpoints are computed.

    line_data : instance of pept.LineData

    &#39;&#39;&#39;

    return self._line_data</code></pre>
</details>
</dd>
<dt id="pept.tracking.peptml.peptml.Cutpoints.max_distance"><code class="name">var <span class="ident">max_distance</span></code></dt>
<dd>
<section class="desc"><p>The maximum distance between any pair of lines for which the cutpoint is considered.</p>
<p>max_distance : float</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">@property
def max_distance(self):
    &#39;&#39;&#39;The maximum distance between any pair of lines for which the cutpoint is considered.

    max_distance : float

    &#39;&#39;&#39;
    return self._max_distance</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="pept.tracking.peptml.peptml.Cutpoints.find_cutpoints"><code class="name flex">
<span>def <span class="ident">find_cutpoints</span></span>(<span>self, line_data, max_distance, cutoffs=None, verbose=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Find the cutpoints of the samples in a <code>LineData</code> instance.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>line_data</code></strong> :&ensp;<code>instance</code> of <a title="pept.LineData" href="../../index.html#pept.LineData"><code>LineData</code></a></dt>
<dd>The LoRs for which the cutpoints will be computed. It is required to be an
instance of <a title="pept.LineData" href="../../index.html#pept.LineData"><code>LineData</code></a>.</dd>
<dt><strong><code>max_distance</code></strong> :&ensp;<code>float</code></dt>
<dd>The maximum distance between any two lines so that their cutpoint will be
considered.</dd>
<dt><strong><code>cutoffs</code></strong> :&ensp;<code>list</code>-<code>like</code> of <code>length</code> <code>6</code>, optional</dt>
<dd>A list (or equivalent) of the cutoff distances for every axis, formatted as
[x_min, x_max, y_min, y_max, z_min, z_max]. Only consider the cutpoints which
fall within these cutoff distances. The default is None, in which case they
are automatically computed using <code>get_cutoffs</code>.</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Provide extra information when computing the cutpoints: time the operation
and show a progress bar. The default is <code>False</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>self</code></strong> :&ensp;<code>the</code> <code>PointData</code> <code>instance</code> of <code>cutpoints</code></dt>
<dd>The computed cutpoints are stored in the <a title="pept.tracking.peptml.peptml.Cutpoints" href="#pept.tracking.peptml.peptml.Cutpoints"><code>Cutpoints</code></a> class, as a
subclass of <a title="pept.PointData" href="../../index.html#pept.PointData"><code>PointData</code></a>.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>Exception</code></dt>
<dd>If <code>line_data</code> is not an instance of <a title="pept.LineData" href="../../index.html#pept.LineData"><code>LineData</code></a>.</dd>
<dt><code>TypeError</code></dt>
<dd>If <code>cutoffs</code> is not a one-dimensional array with values formatted as
<code>[min_x, max_x, min_y, max_y, min_z, max_z]</code>.</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def find_cutpoints(self,
                   line_data,
                   max_distance,
                   cutoffs = None,
                   verbose = False):
    &#39;&#39;&#39;Find the cutpoints of the samples in a `LineData` instance.

    Parameters
    ----------
    line_data : instance of pept.LineData
        The LoRs for which the cutpoints will be computed. It is required to be an
        instance of `pept.LineData`.
    max_distance : float
        The maximum distance between any two lines so that their cutpoint will be
        considered.
    cutoffs : list-like of length 6, optional
        A list (or equivalent) of the cutoff distances for every axis, formatted as
        [x_min, x_max, y_min, y_max, z_min, z_max]. Only consider the cutpoints which
        fall within these cutoff distances. The default is None, in which case they
        are automatically computed using `get_cutoffs`.
    verbose : bool, optional
        Provide extra information when computing the cutpoints: time the operation
        and show a progress bar. The default is `False`.

    Returns
    -------
    self : the PointData instance of cutpoints
        The computed cutpoints are stored in the `Cutpoints` class, as a
        subclass of `pept.PointData`.

    Raises
    ------
    Exception
        If `line_data` is not an instance of `pept.LineData`.
    TypeError
        If `cutoffs` is not a one-dimensional array with values formatted as
        `[min_x, max_x, min_y, max_y, min_z, max_z]`.

    &#39;&#39;&#39;

    if verbose:
        start = time.time()

    # Check line_data is an instance (or a subclass!) of pept.LineData
    if not isinstance(line_data, pept.LineData):
        raise Exception(&#39;[ERROR]: line_data should be an instance of pept.LineData&#39;)

    self._line_data = line_data
    self._max_distance = max_distance

    # If cutoffs were not supplied, compute them
    if cutoffs is None:
        cutoffs = self.get_cutoffs(line_data.line_data)
    # Otherwise make sure they are a C-contiguous numpy array
    else:
        cutoffs = np.asarray(cutoffs, order = &#39;C&#39;, dtype = float)
        if cutoffs.ndim != 1 or len(cutoffs) != 6:
            raise TypeError(&#39;\n[ERROR]: cutoffs should be a one-dimensional array with values [min_x, max_x, min_y, max_y, min_z, max_z]\n&#39;)

    self._cutoffs = cutoffs

    # Using joblib, collect the cutpoints from every sample in a list
    # of arrays. If verbose, show progress bar using tqdm.
    if verbose:
        cutpoints = Parallel(n_jobs = -1, prefer = &#39;threads&#39;)(delayed(self.find_cutpoints_sample)(sample, max_distance, cutoffs) for sample in tqdm(line_data))
    else:
        cutpoints = Parallel(n_jobs = -1, prefer = &#39;threads&#39;)(delayed(self.find_cutpoints_sample)(sample, max_distance, cutoffs) for sample in line_data)

    # cutpoints shape: (n, m, 4), where n is the number of samples, and
    # m is the number of cutpoints in the sample
    cutpoints = np.array(cutpoints)

    number_of_samples = len(cutpoints)
    cutpoints = np.vstack(np.array(cutpoints))
    number_of_cutpoints = len(cutpoints)

    # Average number of cutpoints per sample
    cutpoints_per_sample = int(number_of_cutpoints / number_of_samples)

    super().__init__(cutpoints,
                     sample_size = cutpoints_per_sample,
                     overlap = 0,
                     verbose = False)

    if verbose:
        end = time.time()
        print(&#34;\nFinding the cutpoints took {} seconds\n&#34;.format(end - start))

    return self</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="pept.base.point_data.PointData" href="../../base/point_data.html#pept.base.point_data.PointData">PointData</a></b></code>:
<ul class="hlist">
<li><code><a title="pept.base.point_data.PointData.all_points_trace" href="../../base/point_data.html#pept.base.point_data.PointData.all_points_trace">all_points_trace</a></code></li>
<li><code><a title="pept.base.point_data.PointData.all_points_trace_colorbar" href="../../base/point_data.html#pept.base.point_data.PointData.all_points_trace_colorbar">all_points_trace_colorbar</a></code></li>
<li><code><a title="pept.base.point_data.PointData.number_of_points" href="../../base/point_data.html#pept.base.point_data.PointData.number_of_points">number_of_points</a></code></li>
<li><code><a title="pept.base.point_data.PointData.number_of_samples" href="../../base/point_data.html#pept.base.point_data.PointData.number_of_samples">number_of_samples</a></code></li>
<li><code><a title="pept.base.point_data.PointData.overlap" href="../../base/point_data.html#pept.base.point_data.PointData.overlap">overlap</a></code></li>
<li><code><a title="pept.base.point_data.PointData.plot_all_points" href="../../base/point_data.html#pept.base.point_data.PointData.plot_all_points">plot_all_points</a></code></li>
<li><code><a title="pept.base.point_data.PointData.plot_all_points_alt_axes" href="../../base/point_data.html#pept.base.point_data.PointData.plot_all_points_alt_axes">plot_all_points_alt_axes</a></code></li>
<li><code><a title="pept.base.point_data.PointData.plot_points_sample_n" href="../../base/point_data.html#pept.base.point_data.PointData.plot_points_sample_n">plot_points_sample_n</a></code></li>
<li><code><a title="pept.base.point_data.PointData.plot_points_sample_n_alt_axes" href="../../base/point_data.html#pept.base.point_data.PointData.plot_points_sample_n_alt_axes">plot_points_sample_n_alt_axes</a></code></li>
<li><code><a title="pept.base.point_data.PointData.point_data" href="../../base/point_data.html#pept.base.point_data.PointData.point_data">point_data</a></code></li>
<li><code><a title="pept.base.point_data.PointData.points_sample_n_trace" href="../../base/point_data.html#pept.base.point_data.PointData.points_sample_n_trace">points_sample_n_trace</a></code></li>
<li><code><a title="pept.base.point_data.PointData.points_sample_n_trace_colorbar" href="../../base/point_data.html#pept.base.point_data.PointData.points_sample_n_trace_colorbar">points_sample_n_trace_colorbar</a></code></li>
<li><code><a title="pept.base.point_data.PointData.points_trace" href="../../base/point_data.html#pept.base.point_data.PointData.points_trace">points_trace</a></code></li>
<li><code><a title="pept.base.point_data.PointData.sample_n" href="../../base/point_data.html#pept.base.point_data.PointData.sample_n">sample_n</a></code></li>
<li><code><a title="pept.base.point_data.PointData.sample_size" href="../../base/point_data.html#pept.base.point_data.PointData.sample_size">sample_size</a></code></li>
<li><code><a title="pept.base.point_data.PointData.to_csv" href="../../base/point_data.html#pept.base.point_data.PointData.to_csv">to_csv</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="pept.tracking.peptml.peptml.HDBSCANClusterer"><code class="flex name class">
<span>class <span class="ident">HDBSCANClusterer</span></span>
<span>(</span><span>min_cluster_size=20, min_samples=None, allow_single_cluster=False)</span>
</code></dt>
<dd>
<section class="desc"><p>HDBSCAN-based clustering for cutpoints from LoRs.</p>
<p>This class is a wrapper around the <code>hdbscan</code> package, providing tools for
parallel clustering of samples of cutpoints. It can return <code>PointData</code>
classes which can be easily manipulated or visualised.</p>
<h2 id="parameters">Parameters</h2>
<pre><code>min_cluster_size : int, optional
    (Taken from hdbscan's documentation): The minimum size of clusters;
    single linkage splits that contain fewer points than this will be
    considered points “falling out” of a cluster rather than a cluster
    splitting into two new clusters. The default is 20.
min_samples : int, optional
    (Taken from hdbscan's documentation): The number of samples in a
    neighbourhood for a point to be considered a core point. The default
    is None, being set automatically to the `min_cluster_size`.
allow_single_cluster : bool, optional
    (Taken from hdbscan's documentation): By default HDBSCAN* will not
    produce a single cluster, setting this to True will override this and
    allow single cluster results in the case that you feel this is a valid
    result for your dataset. For PEPT, set this to True if you only have
    one tracer in the dataset. Otherwise, leave it to False, as it will
    provide higher accuracy.
</code></pre>
<h2 id="attributes">Attributes</h2>
<pre><code>min_cluster_size : int
    (Taken from hdbscan's documentation): The minimum size of clusters;
    single linkage splits that contain fewer points than this will be
    considered points “falling out” of a cluster rather than a cluster
    splitting into two new clusters. The default is 20.
min_samples : int
    (Taken from hdbscan's documentation): The number of samples in a
    neighbourhood for a point to be considered a core point. The default
    is None, being set automatically to the `min_cluster_size`.
allow_single_cluster : bool
    (Taken from hdbscan's documentation): By default HDBSCAN* will not
    produce a single cluster, setting this to True will override this and
    allow single cluster results in the case that you feel this is a valid
    result for your dataset. For PEPT, set this to True if you only have
    one tracer in the dataset. Otherwise, leave it to False, as it will
    provide higher accuracy.
</code></pre></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">class HDBSCANClusterer:
    &#39;&#39;&#39;HDBSCAN-based clustering for cutpoints from LoRs.

    This class is a wrapper around the `hdbscan` package, providing tools for
    parallel clustering of samples of cutpoints. It can return `PointData`
    classes which can be easily manipulated or visualised.

    Parameters
    ----------
        min_cluster_size : int, optional
            (Taken from hdbscan&#39;s documentation): The minimum size of clusters;
            single linkage splits that contain fewer points than this will be
            considered points “falling out” of a cluster rather than a cluster
            splitting into two new clusters. The default is 20.
        min_samples : int, optional
            (Taken from hdbscan&#39;s documentation): The number of samples in a
            neighbourhood for a point to be considered a core point. The default
            is None, being set automatically to the `min_cluster_size`.
        allow_single_cluster : bool, optional
            (Taken from hdbscan&#39;s documentation): By default HDBSCAN* will not
            produce a single cluster, setting this to True will override this and
            allow single cluster results in the case that you feel this is a valid
            result for your dataset. For PEPT, set this to True if you only have
            one tracer in the dataset. Otherwise, leave it to False, as it will
            provide higher accuracy.

    Attributes
    ----------
        min_cluster_size : int
            (Taken from hdbscan&#39;s documentation): The minimum size of clusters;
            single linkage splits that contain fewer points than this will be
            considered points “falling out” of a cluster rather than a cluster
            splitting into two new clusters. The default is 20.
        min_samples : int
            (Taken from hdbscan&#39;s documentation): The number of samples in a
            neighbourhood for a point to be considered a core point. The default
            is None, being set automatically to the `min_cluster_size`.
        allow_single_cluster : bool
            (Taken from hdbscan&#39;s documentation): By default HDBSCAN* will not
            produce a single cluster, setting this to True will override this and
            allow single cluster results in the case that you feel this is a valid
            result for your dataset. For PEPT, set this to True if you only have
            one tracer in the dataset. Otherwise, leave it to False, as it will
            provide higher accuracy.

    &#39;&#39;&#39;

    def __init__(self,
                 min_cluster_size = 20,
                 min_samples = None,
                 allow_single_cluster = False):

        if 0 &lt; min_cluster_size &lt; 2:
            print(&#34;\n[WARNING]: min_cluster_size was set to 2, as it was {} &lt; 2\n&#34;.format(min_cluster_size))
            min_cluster_size = 2

        self.clusterer = hdbscan.HDBSCAN(min_cluster_size = min_cluster_size,
                                         min_samples = min_samples,
                                         core_dist_n_jobs = -1,
                                         allow_single_cluster = allow_single_cluster)


    @property
    def min_cluster_size(self):
        return self.clusterer.min_cluster_size


    @min_cluster_size.setter
    def min_cluster_size(self, new_min_cluster_size):
        self.clusterer.min_cluster_size = new_min_cluster_size


    @property
    def min_samples(self):
        return self.clusterer.min_cluster_size


    @min_samples.setter
    def min_samples(self, new_min_samples):
        self.clusterer.min_samples = new_min_samples


    @property
    def allow_single_cluster(self):
        return self.clusterer.allow_single_cluster


    @allow_single_cluster.setter
    def allow_single_cluster(self, option):
        self.clusterer.allow_single_cluster = option


    def fit_sample(self,
                   sample,
                   store_labels = False,
                   noise = False,
                   as_array = True,
                   verbose = False):
        &#39;&#39;&#39;Fit one sample of cutpoints and return the cluster centres and
        (optionally) the labelled cutpoints.

        Parameters
        ----------
        sample : (N, M &gt;= 4) numpy.ndarray
            The sample of points that will be clustered. Every point corresponds to
            a row and is formatted as `[time, x, y, z, etc]`. Only columns `[1, 2, 3]`
            are used for clustering.
        store_labels : bool, optional
            If set to True, the clustered cutpoints are returned along with the centres
            of the clusters. Setting it to False speeds up the clustering. The default
            is False.
        noise : bool, optional
            If set to True, the clustered cutpoints also include the points classified
            as noise. Only has an effect if `store_labels` is set to True. The default
            is False.
        as_array : bool, optional
            If set to True, the centres of the clusters and the clustered cutpoints are
            returned as numpy arrays. If set to False, they are returned inside
            instances of `pept.PointData`.
        verbose : bool, optional
            Provide extra information when computing the cutpoints: time the operation
            and show a progress bar. The default is `False`.

        Returns
        -------
        centres : numpy.ndarray or pept.PointData
            The centroids of every cluster found. They are computed as the average
            of every column of `[time, x, y, z, etc]` of the clustered points. Another
            column is added to the initial data in `sample`, signifying the cluster
            size - the number of points included in the cluster. If `as_array` is
            set to True, it is a numpy array, otherwise the centres are stored
            in a pept.PointData instance.
        clustered_cutpoints : numpy.ndarray or pept.PointData
            The points in `sample` that fall in every cluster. A new column is added
            to the points in `sample` that signifies the label of cluster that the
            point was associated with: all points in cluster number 3 will have the
            number 3 as the last element in their row. The points classified as noise
            have the number -1 associated. If `as_array` is set to True, it is a numpy
            array, otherwise the clustered cutpoints are stored in a pept.PointData
            instance.

        Raises
        ------
        TypeError
            If `sample` is not a numpy array of shape (N, M), where M &gt;= 4.

        &#39;&#39;&#39;

        if verbose:
            start = time.time()

        # sample row: [time, x, y, z]
        if sample.ndim != 2 or sample.shape[1] &lt; 4:
            raise TypeError(&#39;\n[ERROR]: sample should have two dimensions (M, N), where N &gt;= 4. Received {}\n&#39;.format(sample.shape))

        # Only cluster based on [x, y, z]
        labels = self.clusterer.fit_predict(sample[:, 1:4])
        max_label = labels.max()

        centres = []
        clustered_cutpoints = []

        # the centre of a cluster is the average of the time, x, y, z columns
        # and the number of points of that cluster
        # centres row: [time, x, y, z, ..etc.., cluster_size]
        for i in range(0, max_label + 1):
            # Average time, x, y, z of cluster of label i
            centres_row = np.mean(sample[labels == i], axis = 0)
            # Append the number of points of label i =&gt; cluster_size
            centres_row = np.append(centres_row, (labels == i).sum())
            centres.append(centres_row)

        centres = np.array(centres)

        if not as_array:
            centres = pept.PointData(centres,
                                     sample_size = 0,
                                     overlap = 0,
                                     verbose = False)

        # Return all cutpoints as a list of numpy arrays for every label
        # where the last column of an array is the label
        if store_labels:
            # Create a list of numpy arrays with rows: [t, x, y, z, ..etc.., label]
            if noise:
                cutpoints = sample[labels == -1]
                cutpoints = np.insert(cutpoints, cutpoints.shape[1], -1, axis = 1)
                clustered_cutpoints.append(cutpoints)

            for i in range(0, max_label + 1):
                cutpoints = sample[labels == i]
                cutpoints = np.insert(cutpoints, cutpoints.shape[1], i, axis = 1)
                clustered_cutpoints.append(cutpoints)

            clustered_cutpoints = np.vstack(np.array(clustered_cutpoints))

            if not as_array:
                clustered_cutpoints = pept.PointData(clustered_cutpoints,
                                                     sample_size = 0,
                                                     overlap = 0,
                                                     verbose = False)

        if verbose:
            end = time.time()
            print(&#34;Fitting one sample took {} seconds&#34;.format(end - start))

        return [centres, clustered_cutpoints]


    def fit_cutpoints(self,
                      cutpoints,
                      store_labels = False,
                      noise = False,
                      verbose = True):
        &#39;&#39;&#39;Fit cutpoints (an instance of `PointData`) and return the cluster
        centres and (optionally) the labelled cutpoints.

        Parameters
        ----------
        cutpoints : an instance of `pept.PointData`
            The samples of points that will be clustered. In every sample, every point
            corresponds to a row and is formatted as `[time, x, y, z, etc]`. Only
            columns `[1, 2, 3]` are used for clustering.
        store_labels : bool, optional
            If set to True, the clustered cutpoints are returned along with the centres
            of the clusters. Setting it to False speeds up the clustering. The default
            is False.
        noise : bool, optional
            If set to True, the clustered cutpoints also include the points classified
            as noise. Only has an effect if `store_labels` is set to True. The default
            is False.
        verbose : bool, optional
            Provide extra information when computing the cutpoints: time the operation
            and show a progress bar. The default is `False`.

        Returns
        -------
        centres : pept.PointData
            The centroids of every cluster found. They are computed as the average
            of every column of `[time, x, y, z, etc]` of the clustered points. Another
            column is added to the initial data in `sample`, signifying the cluster
            size - the number of points included in the cluster.
        clustered_cutpoints : numpy.ndarray or pept.PointData
            The points in `sample` that fall in every cluster. A new column is added
            to the points in `sample` that signifies the label of cluster that the
            point was associated with: all points in cluster number 3 will have the
            number 3 as the last element in their row. The points classified as noise
            have the number -1 associated.

        Raises
        ------
        Exception
            If `cutpoints` is not an instance (or a subclass) of `pept.PointData`.

        &#39;&#39;&#39;

        if verbose:
            start = time.time()

        if not isinstance(cutpoints, pept.PointData):
            raise Exception(&#39;[ERROR]: cutpoints should be an instance of pept.PointData (or any class inheriting from it)&#39;)

        # Fit all samples in `cutpoints` in parallel using joblib
        # Collect all outputs as a list. If verbose, show progress bar with
        # tqdm
        if verbose:
            data_list = Parallel(n_jobs = -1)(delayed(self.fit_sample)(sample,
                                                store_labels = store_labels,
                                                noise = noise,
                                                as_array = True) for sample in tqdm(cutpoints))
        else:
            data_list = Parallel(n_jobs = -1)(delayed(self.fit_sample)(sample,
                                                store_labels = store_labels,
                                                noise = noise,
                                                as_array = True) for sample in cutpoints)

        # Access joblib.Parallel output as list comprehensions
        centres = np.array([row[0] for row in data_list if len(row[0]) != 0])
        if len(centres) != 0:
            centres = pept.PointData(np.vstack(centres),
                                     sample_size = 0,
                                     overlap = 0,
                                     verbose = False)

        if store_labels:
            clustered_cutpoints = np.array([row[1] for row in data_list if len(row[1]) != 0])
            clustered_cutpoints = pept.PointData(np.vstack(np.array(clustered_cutpoints)),
                                                 sample_size = 0,
                                                 overlap = 0,
                                                 verbose = False)

        if verbose:
            end = time.time()
            print(&#34;\nFitting cutpoints took {} seconds\n&#34;.format(end - start))

        if store_labels:
            return [centres, clustered_cutpoints]
        else:
            return [centres, []]</code></pre>
</details>
<h3>Instance variables</h3>
<dl>
<dt id="pept.tracking.peptml.peptml.HDBSCANClusterer.allow_single_cluster"><code class="name">var <span class="ident">allow_single_cluster</span></code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">@property
def allow_single_cluster(self):
    return self.clusterer.allow_single_cluster</code></pre>
</details>
</dd>
<dt id="pept.tracking.peptml.peptml.HDBSCANClusterer.min_cluster_size"><code class="name">var <span class="ident">min_cluster_size</span></code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">@property
def min_cluster_size(self):
    return self.clusterer.min_cluster_size</code></pre>
</details>
</dd>
<dt id="pept.tracking.peptml.peptml.HDBSCANClusterer.min_samples"><code class="name">var <span class="ident">min_samples</span></code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">@property
def min_samples(self):
    return self.clusterer.min_cluster_size</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="pept.tracking.peptml.peptml.HDBSCANClusterer.fit_cutpoints"><code class="name flex">
<span>def <span class="ident">fit_cutpoints</span></span>(<span>self, cutpoints, store_labels=False, noise=False, verbose=True)</span>
</code></dt>
<dd>
<section class="desc"><p>Fit cutpoints (an instance of <code>PointData</code>) and return the cluster
centres and (optionally) the labelled cutpoints.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>cutpoints</code></strong> :&ensp;<code>an</code> <code>instance</code> of <code>&lt;a title="pept.PointData" href="../../index.html#pept.PointData"&gt;</code>PointData<code>&lt;/a&gt;</code></dt>
<dd>The samples of points that will be clustered. In every sample, every point
corresponds to a row and is formatted as <code>[time, x, y, z, etc]</code>. Only
columns <code>[1, 2, 3]</code> are used for clustering.</dd>
<dt><strong><code>store_labels</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>If set to True, the clustered cutpoints are returned along with the centres
of the clusters. Setting it to False speeds up the clustering. The default
is False.</dd>
<dt><strong><code>noise</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>If set to True, the clustered cutpoints also include the points classified
as noise. Only has an effect if <code>store_labels</code> is set to True. The default
is False.</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Provide extra information when computing the cutpoints: time the operation
and show a progress bar. The default is <code>False</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>centres</code></strong> :&ensp;<code>pept.PointData</code></dt>
<dd>The centroids of every cluster found. They are computed as the average
of every column of <code>[time, x, y, z, etc]</code> of the clustered points. Another
column is added to the initial data in <code>sample</code>, signifying the cluster
size - the number of points included in the cluster.</dd>
<dt><strong><code>clustered_cutpoints</code></strong> :&ensp;<code>numpy.ndarray</code> or <code>pept.PointData</code></dt>
<dd>The points in <code>sample</code> that fall in every cluster. A new column is added
to the points in <code>sample</code> that signifies the label of cluster that the
point was associated with: all points in cluster number 3 will have the
number 3 as the last element in their row. The points classified as noise
have the number -1 associated.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>Exception</code></dt>
<dd>If <code>cutpoints</code> is not an instance (or a subclass) of <code>pept.PointData</code>.</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def fit_cutpoints(self,
                  cutpoints,
                  store_labels = False,
                  noise = False,
                  verbose = True):
    &#39;&#39;&#39;Fit cutpoints (an instance of `PointData`) and return the cluster
    centres and (optionally) the labelled cutpoints.

    Parameters
    ----------
    cutpoints : an instance of `pept.PointData`
        The samples of points that will be clustered. In every sample, every point
        corresponds to a row and is formatted as `[time, x, y, z, etc]`. Only
        columns `[1, 2, 3]` are used for clustering.
    store_labels : bool, optional
        If set to True, the clustered cutpoints are returned along with the centres
        of the clusters. Setting it to False speeds up the clustering. The default
        is False.
    noise : bool, optional
        If set to True, the clustered cutpoints also include the points classified
        as noise. Only has an effect if `store_labels` is set to True. The default
        is False.
    verbose : bool, optional
        Provide extra information when computing the cutpoints: time the operation
        and show a progress bar. The default is `False`.

    Returns
    -------
    centres : pept.PointData
        The centroids of every cluster found. They are computed as the average
        of every column of `[time, x, y, z, etc]` of the clustered points. Another
        column is added to the initial data in `sample`, signifying the cluster
        size - the number of points included in the cluster.
    clustered_cutpoints : numpy.ndarray or pept.PointData
        The points in `sample` that fall in every cluster. A new column is added
        to the points in `sample` that signifies the label of cluster that the
        point was associated with: all points in cluster number 3 will have the
        number 3 as the last element in their row. The points classified as noise
        have the number -1 associated.

    Raises
    ------
    Exception
        If `cutpoints` is not an instance (or a subclass) of `pept.PointData`.

    &#39;&#39;&#39;

    if verbose:
        start = time.time()

    if not isinstance(cutpoints, pept.PointData):
        raise Exception(&#39;[ERROR]: cutpoints should be an instance of pept.PointData (or any class inheriting from it)&#39;)

    # Fit all samples in `cutpoints` in parallel using joblib
    # Collect all outputs as a list. If verbose, show progress bar with
    # tqdm
    if verbose:
        data_list = Parallel(n_jobs = -1)(delayed(self.fit_sample)(sample,
                                            store_labels = store_labels,
                                            noise = noise,
                                            as_array = True) for sample in tqdm(cutpoints))
    else:
        data_list = Parallel(n_jobs = -1)(delayed(self.fit_sample)(sample,
                                            store_labels = store_labels,
                                            noise = noise,
                                            as_array = True) for sample in cutpoints)

    # Access joblib.Parallel output as list comprehensions
    centres = np.array([row[0] for row in data_list if len(row[0]) != 0])
    if len(centres) != 0:
        centres = pept.PointData(np.vstack(centres),
                                 sample_size = 0,
                                 overlap = 0,
                                 verbose = False)

    if store_labels:
        clustered_cutpoints = np.array([row[1] for row in data_list if len(row[1]) != 0])
        clustered_cutpoints = pept.PointData(np.vstack(np.array(clustered_cutpoints)),
                                             sample_size = 0,
                                             overlap = 0,
                                             verbose = False)

    if verbose:
        end = time.time()
        print(&#34;\nFitting cutpoints took {} seconds\n&#34;.format(end - start))

    if store_labels:
        return [centres, clustered_cutpoints]
    else:
        return [centres, []]</code></pre>
</details>
</dd>
<dt id="pept.tracking.peptml.peptml.HDBSCANClusterer.fit_sample"><code class="name flex">
<span>def <span class="ident">fit_sample</span></span>(<span>self, sample, store_labels=False, noise=False, as_array=True, verbose=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Fit one sample of cutpoints and return the cluster centres and
(optionally) the labelled cutpoints.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>sample</code></strong> :&ensp;(<code>N</code>, <code>M</code> &gt;= <code>4</code>) <code>numpy.ndarray</code></dt>
<dd>The sample of points that will be clustered. Every point corresponds to
a row and is formatted as <code>[time, x, y, z, etc]</code>. Only columns <code>[1, 2, 3]</code>
are used for clustering.</dd>
<dt><strong><code>store_labels</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>If set to True, the clustered cutpoints are returned along with the centres
of the clusters. Setting it to False speeds up the clustering. The default
is False.</dd>
<dt><strong><code>noise</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>If set to True, the clustered cutpoints also include the points classified
as noise. Only has an effect if <code>store_labels</code> is set to True. The default
is False.</dd>
<dt><strong><code>as_array</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>If set to True, the centres of the clusters and the clustered cutpoints are
returned as numpy arrays. If set to False, they are returned inside
instances of <a title="pept.PointData" href="../../index.html#pept.PointData"><code>PointData</code></a>.</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Provide extra information when computing the cutpoints: time the operation
and show a progress bar. The default is <code>False</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>centres</code></strong> :&ensp;<code>numpy.ndarray</code> or <a title="pept.PointData" href="../../index.html#pept.PointData"><code>PointData</code></a></dt>
<dd>The centroids of every cluster found. They are computed as the average
of every column of <code>[time, x, y, z, etc]</code> of the clustered points. Another
column is added to the initial data in <code>sample</code>, signifying the cluster
size - the number of points included in the cluster. If <code>as_array</code> is
set to True, it is a numpy array, otherwise the centres are stored
in a pept.PointData instance.</dd>
<dt><strong><code>clustered_cutpoints</code></strong> :&ensp;<code>numpy.ndarray</code> or <a title="pept.PointData" href="../../index.html#pept.PointData"><code>PointData</code></a></dt>
<dd>The points in <code>sample</code> that fall in every cluster. A new column is added
to the points in <code>sample</code> that signifies the label of cluster that the
point was associated with: all points in cluster number 3 will have the
number 3 as the last element in their row. The points classified as noise
have the number -1 associated. If <code>as_array</code> is set to True, it is a numpy
array, otherwise the clustered cutpoints are stored in a pept.PointData
instance.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>TypeError</code></dt>
<dd>If <code>sample</code> is not a numpy array of shape (N, M), where M &gt;= 4.</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def fit_sample(self,
               sample,
               store_labels = False,
               noise = False,
               as_array = True,
               verbose = False):
    &#39;&#39;&#39;Fit one sample of cutpoints and return the cluster centres and
    (optionally) the labelled cutpoints.

    Parameters
    ----------
    sample : (N, M &gt;= 4) numpy.ndarray
        The sample of points that will be clustered. Every point corresponds to
        a row and is formatted as `[time, x, y, z, etc]`. Only columns `[1, 2, 3]`
        are used for clustering.
    store_labels : bool, optional
        If set to True, the clustered cutpoints are returned along with the centres
        of the clusters. Setting it to False speeds up the clustering. The default
        is False.
    noise : bool, optional
        If set to True, the clustered cutpoints also include the points classified
        as noise. Only has an effect if `store_labels` is set to True. The default
        is False.
    as_array : bool, optional
        If set to True, the centres of the clusters and the clustered cutpoints are
        returned as numpy arrays. If set to False, they are returned inside
        instances of `pept.PointData`.
    verbose : bool, optional
        Provide extra information when computing the cutpoints: time the operation
        and show a progress bar. The default is `False`.

    Returns
    -------
    centres : numpy.ndarray or pept.PointData
        The centroids of every cluster found. They are computed as the average
        of every column of `[time, x, y, z, etc]` of the clustered points. Another
        column is added to the initial data in `sample`, signifying the cluster
        size - the number of points included in the cluster. If `as_array` is
        set to True, it is a numpy array, otherwise the centres are stored
        in a pept.PointData instance.
    clustered_cutpoints : numpy.ndarray or pept.PointData
        The points in `sample` that fall in every cluster. A new column is added
        to the points in `sample` that signifies the label of cluster that the
        point was associated with: all points in cluster number 3 will have the
        number 3 as the last element in their row. The points classified as noise
        have the number -1 associated. If `as_array` is set to True, it is a numpy
        array, otherwise the clustered cutpoints are stored in a pept.PointData
        instance.

    Raises
    ------
    TypeError
        If `sample` is not a numpy array of shape (N, M), where M &gt;= 4.

    &#39;&#39;&#39;

    if verbose:
        start = time.time()

    # sample row: [time, x, y, z]
    if sample.ndim != 2 or sample.shape[1] &lt; 4:
        raise TypeError(&#39;\n[ERROR]: sample should have two dimensions (M, N), where N &gt;= 4. Received {}\n&#39;.format(sample.shape))

    # Only cluster based on [x, y, z]
    labels = self.clusterer.fit_predict(sample[:, 1:4])
    max_label = labels.max()

    centres = []
    clustered_cutpoints = []

    # the centre of a cluster is the average of the time, x, y, z columns
    # and the number of points of that cluster
    # centres row: [time, x, y, z, ..etc.., cluster_size]
    for i in range(0, max_label + 1):
        # Average time, x, y, z of cluster of label i
        centres_row = np.mean(sample[labels == i], axis = 0)
        # Append the number of points of label i =&gt; cluster_size
        centres_row = np.append(centres_row, (labels == i).sum())
        centres.append(centres_row)

    centres = np.array(centres)

    if not as_array:
        centres = pept.PointData(centres,
                                 sample_size = 0,
                                 overlap = 0,
                                 verbose = False)

    # Return all cutpoints as a list of numpy arrays for every label
    # where the last column of an array is the label
    if store_labels:
        # Create a list of numpy arrays with rows: [t, x, y, z, ..etc.., label]
        if noise:
            cutpoints = sample[labels == -1]
            cutpoints = np.insert(cutpoints, cutpoints.shape[1], -1, axis = 1)
            clustered_cutpoints.append(cutpoints)

        for i in range(0, max_label + 1):
            cutpoints = sample[labels == i]
            cutpoints = np.insert(cutpoints, cutpoints.shape[1], i, axis = 1)
            clustered_cutpoints.append(cutpoints)

        clustered_cutpoints = np.vstack(np.array(clustered_cutpoints))

        if not as_array:
            clustered_cutpoints = pept.PointData(clustered_cutpoints,
                                                 sample_size = 0,
                                                 overlap = 0,
                                                 verbose = False)

    if verbose:
        end = time.time()
        print(&#34;Fitting one sample took {} seconds&#34;.format(end - start))

    return [centres, clustered_cutpoints]</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="pept.tracking.peptml.peptml.TrajectorySeparation"><code class="flex name class">
<span>class <span class="ident">TrajectorySeparation</span></span>
<span>(</span><span>centres, pointsToCheck=25, maxDistance=20, maxClusterDiff=500)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">class TrajectorySeparation:

    def __init__(self, centres, pointsToCheck = 25, maxDistance = 20, maxClusterDiff = 500):
        # centres row: [time, x, y, z, clusterSize]
        # Make sure the trajectory is memory-contiguous for efficient
        # KDTree partitioning
        self.centres = np.ascontiguousarray(centres)
        self.pointsToCheck = pointsToCheck
        self.maxDistance = maxDistance
        self.maxClusterDiff = maxClusterDiff

        # For every point in centres, save a set of the trajectory
        # indices of the trajectories that they are part of
        #   eg. centres[2] is part of trajectories 0 and 1 =&gt;
        #   trajectoryIndices[2] = {0, 1}
        # Initialise a vector of empty sets of size len(centres)
        self.trajectoryIndices = np.array([ set() for i in range(len(self.centres)) ])

        # For every trajectory found, save a list of the indices of
        # the centres that are part of that trajectory
        #   eg. trajectory 1 is comprised of centres 3, 5 and 8 =&gt;
        #   centresIndices[1] = [3, 5, 8]
        self.centresIndices = [[]]

        # Maximum trajectory index
        self.maxIndex = 0


    def findTrajectories(self):

        for i, currentPoint in enumerate(self.centres):

            if i == 0:
                # Add the first point to trajectory 0
                self.trajectoryIndices[0].add(self.maxIndex)
                self.centresIndices[self.maxIndex].append(0)
                self.maxIndex += 1
                continue

            # Search for the closest previous pointsToCheck points
            # within a given maxDistance
            startIndex = i - self.pointsToCheck
            endIndex = i

            if startIndex &lt; 0:
                startIndex = 0

            # Construct a KDTree from the x, y, z (1:4) of the
            # selected points. Get the indices for all the points within
            # maxDistance of the currentPoint
            tree = cKDTree(self.centres[startIndex:endIndex, 1:4])
            closestIndices = tree.query_ball_point(currentPoint[1:4], self.maxDistance, n_jobs=-1)
            closestIndices = np.array(closestIndices) + startIndex

            # If no point was found, it is a new trajectory. Continue
            if len(closestIndices) == 0:
                self.trajectoryIndices[i].add(self.maxIndex)
                self.centresIndices.append([i])
                self.maxIndex += 1
                continue

            # For every close point found, search for all the trajectory indices
            #   - If all trajectory indices sets are equal and of a single value
            #   then currentPoint is part of the same trajectory
            #   - If all trajectory indices sets are equal, but of more values,
            #   then currentPoint diverged from an intersection of trajectories
            #   and is part of a single trajectory =&gt; separate it
            #
            #   - If every pair of trajectory indices sets is not disjoint, then
            #   currentPoint is only one of them
            #   - If there exists a pair of trajectory indices sets that is
            #   disjoint, then currentPoint is part of all of them

            # Select the trajectories of all the points that were found
            # to be the closest
            closestTrajectories = self.trajectoryIndices[closestIndices]
            #print(&#34;closestTrajectories:&#34;)
            #print(closestTrajectories)

            # If all the closest points are part of the same trajectory
            # (just one!), then the currentPoint is part of it too
            if (np.all(closestTrajectories == closestTrajectories[0]) and
                len(closestTrajectories[0]) == 1):

                self.trajectoryIndices[i] = closestTrajectories[0]
                self.centresIndices[ next(iter(closestTrajectories[0])) ].append(i)
                continue

            # Otherwise, check the points based on their cluster size
            else:
                # Create a list of all the trajectories that were found to
                # intersect
                #print(&#39;\nIntersection:&#39;)
                closestTrajIndices = list( set().union(*closestTrajectories) )

                #print(&#34;ClosestTrajIndices:&#34;)
                #print(closestTrajIndices)

                # For each close trajectory, calculate the mean cluster size
                # of the last lastPoints points
                lastPoints = 50

                # Keep track of the mean cluster size that is the closest to
                # the currentPoint&#39;s clusterSize
                currentClusterSize = currentPoint[4]
                #print(&#34;currentClusterSize = {}&#34;.format(currentClusterSize))
                closestTrajIndex = -1
                clusterSizeDiff = self.maxClusterDiff

                for trajIndex in closestTrajIndices:
                    #print(&#34;trajIndex = {}&#34;.format(trajIndex))

                    trajCentres = self.centres[ self.centresIndices[trajIndex] ]
                    #print(&#34;trajCentres:&#34;)
                    #print(trajCentres)
                    meanClusterSize = trajCentres[-lastPoints:][:, 4].mean()
                    #print(&#34;meanClusterSize = {}&#34;.format(meanClusterSize))
                    #print(&#34;clusterSizeDiff = {}&#34;.format(clusterSizeDiff))
                    #print(&#34;abs diff = {}&#34;.format(np.abs( currentClusterSize - meanClusterSize )))
                    if np.abs( currentClusterSize - meanClusterSize ) &lt; clusterSizeDiff:
                        closestTrajIndex = trajIndex
                        clusterSizeDiff = np.abs( currentClusterSize - meanClusterSize )

                if closestTrajIndex == -1:
                    #self.trajectoryIndices[i] = set(closestTrajIndices)
                    #for trajIndex in closestTrajIndices:
                    #    self.centresIndices[trajIndex].append(i)

                    print(&#34;\n**** -1 ****\n&#34;)
                    break
                else:
                    #print(&#34;ClosestTrajIndex found = {}&#34;.format(closestTrajIndex))
                    self.trajectoryIndices[i] = set([closestTrajIndex])
                    self.centresIndices[closestTrajIndex].append(i)




            &#39;&#39;&#39;
            # If the current point is not part of any trajectory, assign it
            # the maxIndex and increment it
            if len(self.trajectoryIndices[i]) == 0:
                self.trajectoryIndices[i].append(self.maxIndex)
                self.maxIndex += 1

            print(self.trajectoryIndices[i])
            print(self.maxIndex)

            # Construct a KDTree from the numberOfPoints in front of
            # the current point
            tree = cKDTree(self.trajectory[(i + 1):(i + self.numberOfPoints + 2)][1:4])

            # For every trajectory that the current point is part of,
            # find the closest points in front of it
            numberOfIntersections = len(self.trajectoryIndices[i])
            dist, nextPointsIndices = tree.query(currentPoint, k=numberOfIntersections, distance_upper_bound=self.maxDistance, n_jobs=-1)

            print(nextPointsIndices)

            # If the current point is part of more trajectories,
            # an intersection happened. Call subroutine to part
            # the trajectories
            if numberOfIntersections &gt; 1:
                for j in range(0, len(self.trajectoryIndices[i])):
                    trajIndex = self.trajectoryIndices[i][j]
                    self.trajectoryIndices[i + 1 + nextPointsIndices[j]].append(trajIndex)

            else:
                self.trajectoryIndices[i + 1 + nextPointsIndices].append(self.trajectoryIndices[i][0])

            print(self.trajectoryIndices)
            &#39;&#39;&#39;


    def getTrajectories(self):

        self.individualTrajectories = []
        for trajCentres in self.centresIndices:
            self.individualTrajectories.append(self.centres[trajCentres])

        self.individualTrajectories = np.array(self.individualTrajectories)
        return self.individualTrajectories

        &#39;&#39;&#39;
        self.individualTrajectories = [ [] for i in range(0, self.maxIndex + 1) ]
        for i in range(0, len(self.trajectoryIndices)):
            for trajIndex in self.trajectoryIndices[i]:
                self.individualTrajectories[trajIndex].append(self.centres[i])

        self.individualTrajectories = np.array(self.individualTrajectories)
        for i in range(len(self.individualTrajectories)):
            if len(self.individualTrajectories[i]) &gt; 0:
                self.individualTrajectories[i] = np.vstack(self.individualTrajectories[i])
        return self.individualTrajectories
        &#39;&#39;&#39;


    def plotTrajectoriesAltAxes(self, ax):
        trajectories = self.getTrajectories()
        for traj in trajectories:
            if len(traj) &gt; 0:
                ax.scatter(traj[:, 3], traj[:, 1], traj[:, 2], marker=&#39;D&#39;, s=10)</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="pept.tracking.peptml.peptml.TrajectorySeparation.findTrajectories"><code class="name flex">
<span>def <span class="ident">findTrajectories</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def findTrajectories(self):

    for i, currentPoint in enumerate(self.centres):

        if i == 0:
            # Add the first point to trajectory 0
            self.trajectoryIndices[0].add(self.maxIndex)
            self.centresIndices[self.maxIndex].append(0)
            self.maxIndex += 1
            continue

        # Search for the closest previous pointsToCheck points
        # within a given maxDistance
        startIndex = i - self.pointsToCheck
        endIndex = i

        if startIndex &lt; 0:
            startIndex = 0

        # Construct a KDTree from the x, y, z (1:4) of the
        # selected points. Get the indices for all the points within
        # maxDistance of the currentPoint
        tree = cKDTree(self.centres[startIndex:endIndex, 1:4])
        closestIndices = tree.query_ball_point(currentPoint[1:4], self.maxDistance, n_jobs=-1)
        closestIndices = np.array(closestIndices) + startIndex

        # If no point was found, it is a new trajectory. Continue
        if len(closestIndices) == 0:
            self.trajectoryIndices[i].add(self.maxIndex)
            self.centresIndices.append([i])
            self.maxIndex += 1
            continue

        # For every close point found, search for all the trajectory indices
        #   - If all trajectory indices sets are equal and of a single value
        #   then currentPoint is part of the same trajectory
        #   - If all trajectory indices sets are equal, but of more values,
        #   then currentPoint diverged from an intersection of trajectories
        #   and is part of a single trajectory =&gt; separate it
        #
        #   - If every pair of trajectory indices sets is not disjoint, then
        #   currentPoint is only one of them
        #   - If there exists a pair of trajectory indices sets that is
        #   disjoint, then currentPoint is part of all of them

        # Select the trajectories of all the points that were found
        # to be the closest
        closestTrajectories = self.trajectoryIndices[closestIndices]
        #print(&#34;closestTrajectories:&#34;)
        #print(closestTrajectories)

        # If all the closest points are part of the same trajectory
        # (just one!), then the currentPoint is part of it too
        if (np.all(closestTrajectories == closestTrajectories[0]) and
            len(closestTrajectories[0]) == 1):

            self.trajectoryIndices[i] = closestTrajectories[0]
            self.centresIndices[ next(iter(closestTrajectories[0])) ].append(i)
            continue

        # Otherwise, check the points based on their cluster size
        else:
            # Create a list of all the trajectories that were found to
            # intersect
            #print(&#39;\nIntersection:&#39;)
            closestTrajIndices = list( set().union(*closestTrajectories) )

            #print(&#34;ClosestTrajIndices:&#34;)
            #print(closestTrajIndices)

            # For each close trajectory, calculate the mean cluster size
            # of the last lastPoints points
            lastPoints = 50

            # Keep track of the mean cluster size that is the closest to
            # the currentPoint&#39;s clusterSize
            currentClusterSize = currentPoint[4]
            #print(&#34;currentClusterSize = {}&#34;.format(currentClusterSize))
            closestTrajIndex = -1
            clusterSizeDiff = self.maxClusterDiff

            for trajIndex in closestTrajIndices:
                #print(&#34;trajIndex = {}&#34;.format(trajIndex))

                trajCentres = self.centres[ self.centresIndices[trajIndex] ]
                #print(&#34;trajCentres:&#34;)
                #print(trajCentres)
                meanClusterSize = trajCentres[-lastPoints:][:, 4].mean()
                #print(&#34;meanClusterSize = {}&#34;.format(meanClusterSize))
                #print(&#34;clusterSizeDiff = {}&#34;.format(clusterSizeDiff))
                #print(&#34;abs diff = {}&#34;.format(np.abs( currentClusterSize - meanClusterSize )))
                if np.abs( currentClusterSize - meanClusterSize ) &lt; clusterSizeDiff:
                    closestTrajIndex = trajIndex
                    clusterSizeDiff = np.abs( currentClusterSize - meanClusterSize )

            if closestTrajIndex == -1:
                #self.trajectoryIndices[i] = set(closestTrajIndices)
                #for trajIndex in closestTrajIndices:
                #    self.centresIndices[trajIndex].append(i)

                print(&#34;\n**** -1 ****\n&#34;)
                break
            else:
                #print(&#34;ClosestTrajIndex found = {}&#34;.format(closestTrajIndex))
                self.trajectoryIndices[i] = set([closestTrajIndex])
                self.centresIndices[closestTrajIndex].append(i)




        &#39;&#39;&#39;
        # If the current point is not part of any trajectory, assign it
        # the maxIndex and increment it
        if len(self.trajectoryIndices[i]) == 0:
            self.trajectoryIndices[i].append(self.maxIndex)
            self.maxIndex += 1

        print(self.trajectoryIndices[i])
        print(self.maxIndex)

        # Construct a KDTree from the numberOfPoints in front of
        # the current point
        tree = cKDTree(self.trajectory[(i + 1):(i + self.numberOfPoints + 2)][1:4])

        # For every trajectory that the current point is part of,
        # find the closest points in front of it
        numberOfIntersections = len(self.trajectoryIndices[i])
        dist, nextPointsIndices = tree.query(currentPoint, k=numberOfIntersections, distance_upper_bound=self.maxDistance, n_jobs=-1)

        print(nextPointsIndices)

        # If the current point is part of more trajectories,
        # an intersection happened. Call subroutine to part
        # the trajectories
        if numberOfIntersections &gt; 1:
            for j in range(0, len(self.trajectoryIndices[i])):
                trajIndex = self.trajectoryIndices[i][j]
                self.trajectoryIndices[i + 1 + nextPointsIndices[j]].append(trajIndex)

        else:
            self.trajectoryIndices[i + 1 + nextPointsIndices].append(self.trajectoryIndices[i][0])

        print(self.trajectoryIndices)
        &#39;&#39;&#39;</code></pre>
</details>
</dd>
<dt id="pept.tracking.peptml.peptml.TrajectorySeparation.getTrajectories"><code class="name flex">
<span>def <span class="ident">getTrajectories</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def getTrajectories(self):

    self.individualTrajectories = []
    for trajCentres in self.centresIndices:
        self.individualTrajectories.append(self.centres[trajCentres])

    self.individualTrajectories = np.array(self.individualTrajectories)
    return self.individualTrajectories

    &#39;&#39;&#39;
    self.individualTrajectories = [ [] for i in range(0, self.maxIndex + 1) ]
    for i in range(0, len(self.trajectoryIndices)):
        for trajIndex in self.trajectoryIndices[i]:
            self.individualTrajectories[trajIndex].append(self.centres[i])

    self.individualTrajectories = np.array(self.individualTrajectories)
    for i in range(len(self.individualTrajectories)):
        if len(self.individualTrajectories[i]) &gt; 0:
            self.individualTrajectories[i] = np.vstack(self.individualTrajectories[i])
    return self.individualTrajectories
    &#39;&#39;&#39;</code></pre>
</details>
</dd>
<dt id="pept.tracking.peptml.peptml.TrajectorySeparation.plotTrajectoriesAltAxes"><code class="name flex">
<span>def <span class="ident">plotTrajectoriesAltAxes</span></span>(<span>self, ax)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def plotTrajectoriesAltAxes(self, ax):
    trajectories = self.getTrajectories()
    for traj in trajectories:
        if len(traj) &gt; 0:
            ax.scatter(traj[:, 3], traj[:, 1], traj[:, 2], marker=&#39;D&#39;, s=10)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="pept.tracking.peptml" href="index.html">pept.tracking.peptml</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="pept.tracking.peptml.peptml.findMeanError" href="#pept.tracking.peptml.peptml.findMeanError">findMeanError</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="pept.tracking.peptml.peptml.Cutpoints" href="#pept.tracking.peptml.peptml.Cutpoints">Cutpoints</a></code></h4>
<ul class="">
<li><code><a title="pept.tracking.peptml.peptml.Cutpoints.cutoffs" href="#pept.tracking.peptml.peptml.Cutpoints.cutoffs">cutoffs</a></code></li>
<li><code><a title="pept.tracking.peptml.peptml.Cutpoints.find_cutpoints" href="#pept.tracking.peptml.peptml.Cutpoints.find_cutpoints">find_cutpoints</a></code></li>
<li><code><a title="pept.tracking.peptml.peptml.Cutpoints.find_cutpoints_sample" href="#pept.tracking.peptml.peptml.Cutpoints.find_cutpoints_sample">find_cutpoints_sample</a></code></li>
<li><code><a title="pept.tracking.peptml.peptml.Cutpoints.get_cutoffs" href="#pept.tracking.peptml.peptml.Cutpoints.get_cutoffs">get_cutoffs</a></code></li>
<li><code><a title="pept.tracking.peptml.peptml.Cutpoints.line_data" href="#pept.tracking.peptml.peptml.Cutpoints.line_data">line_data</a></code></li>
<li><code><a title="pept.tracking.peptml.peptml.Cutpoints.max_distance" href="#pept.tracking.peptml.peptml.Cutpoints.max_distance">max_distance</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="pept.tracking.peptml.peptml.HDBSCANClusterer" href="#pept.tracking.peptml.peptml.HDBSCANClusterer">HDBSCANClusterer</a></code></h4>
<ul class="">
<li><code><a title="pept.tracking.peptml.peptml.HDBSCANClusterer.allow_single_cluster" href="#pept.tracking.peptml.peptml.HDBSCANClusterer.allow_single_cluster">allow_single_cluster</a></code></li>
<li><code><a title="pept.tracking.peptml.peptml.HDBSCANClusterer.fit_cutpoints" href="#pept.tracking.peptml.peptml.HDBSCANClusterer.fit_cutpoints">fit_cutpoints</a></code></li>
<li><code><a title="pept.tracking.peptml.peptml.HDBSCANClusterer.fit_sample" href="#pept.tracking.peptml.peptml.HDBSCANClusterer.fit_sample">fit_sample</a></code></li>
<li><code><a title="pept.tracking.peptml.peptml.HDBSCANClusterer.min_cluster_size" href="#pept.tracking.peptml.peptml.HDBSCANClusterer.min_cluster_size">min_cluster_size</a></code></li>
<li><code><a title="pept.tracking.peptml.peptml.HDBSCANClusterer.min_samples" href="#pept.tracking.peptml.peptml.HDBSCANClusterer.min_samples">min_samples</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="pept.tracking.peptml.peptml.TrajectorySeparation" href="#pept.tracking.peptml.peptml.TrajectorySeparation">TrajectorySeparation</a></code></h4>
<ul class="">
<li><code><a title="pept.tracking.peptml.peptml.TrajectorySeparation.findTrajectories" href="#pept.tracking.peptml.peptml.TrajectorySeparation.findTrajectories">findTrajectories</a></code></li>
<li><code><a title="pept.tracking.peptml.peptml.TrajectorySeparation.getTrajectories" href="#pept.tracking.peptml.peptml.TrajectorySeparation.getTrajectories">getTrajectories</a></code></li>
<li><code><a title="pept.tracking.peptml.peptml.TrajectorySeparation.plotTrajectoriesAltAxes" href="#pept.tracking.peptml.peptml.TrajectorySeparation.plotTrajectoriesAltAxes">plotTrajectoriesAltAxes</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.6.3</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>